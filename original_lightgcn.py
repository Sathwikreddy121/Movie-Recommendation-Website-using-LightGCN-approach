# -*- coding: utf-8 -*-
"""originial_lightgcn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lY_USD3aYOaVYIDk76P5y0W9NeaD7D3v
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import random
import torch
import string
import re
import matplotlib.pyplot as plt

params = {'legend.fontsize': 'medium',
          'figure.figsize': (10, 8),
          'figure.dpi': 100,
         'axes.labelsize': 'medium',
         'axes.titlesize':'medium',
         'xtick.labelsize':'medium',
         'ytick.labelsize':'medium'}
plt.rcParams.update(params)

# Standard library imports
import random
import time

# Third-party imports
import networkx as nx
pd.set_option('display.max_colwidth', None)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
import torch_geometric
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import degree

from tqdm.notebook import tqdm
from sklearn import preprocessing as pp
from sklearn.model_selection import train_test_split
import scipy.sparse as sp

torch_geometric.__version__

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

movies_df = pd.read_csv("C:\\Users\\HP\\Documents\\Major_Project_Files\\frontend\\ml-latest-small\\ml-latest-small\\movies.csv",index_col='movieId')
ratings_df = pd.read_csv("C:\\Users\\HP\\Documents\\Major_Project_Files\\frontend\\ml-latest-small\\ml-latest-small\\ratings.csv")


ratings_df.shape

# How many ratings are a 3 or above?
ratings_df = ratings_df[ratings_df['rating']>=3]


# What's the distribution of highly rated movies?

ratings_df.groupby(['rating'])['rating'].count()

# Perform a 80/20 train-test split on the interactions in the dataset
train, test = train_test_split(ratings_df.values, test_size=0.2, random_state=16)
train_df = pd.DataFrame(train, columns=ratings_df.columns)
test_df = pd.DataFrame(test, columns=ratings_df.columns)

le_user = pp.LabelEncoder()
le_item = pp.LabelEncoder()
train_df['user_id_idx'] = le_user.fit_transform(train_df['userId'].values)
train_df['item_id_idx'] = le_item.fit_transform(train_df['movieId'].values)

train_user_ids = train_df['userId'].unique()
train_item_ids = train_df['movieId'].unique()


test_df = test_df[
  (test_df['userId'].isin(train_user_ids)) & \
  (test_df['movieId'].isin(train_item_ids))
]


test_df['user_id_idx'] = le_user.transform(test_df['userId'].values)
test_df['item_id_idx'] = le_item.transform(test_df['movieId'].values)

n_users = train_df['user_id_idx'].nunique()
n_items = train_df['item_id_idx'].nunique()


def data_loader(data, batch_size, n_usr, n_itm):

    def sample_neg(x):
        while True:
            neg_id = random.randint(0, n_itm - 1)
            if neg_id not in x:
                return neg_id

    interected_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()
    indices = [x for x in range(n_usr)]

    if n_usr < batch_size:
        users = [random.choice(indices) for _ in range(batch_size)]
    else:
        users = random.sample(indices, batch_size)
    users.sort()
    users_df = pd.DataFrame(users,columns = ['users'])

    interected_items_df = pd.merge(interected_items_df, users_df, how = 'right', left_on = 'user_id_idx', right_on = 'users')
    pos_items = interected_items_df['item_id_idx'].apply(lambda x : random.choice(x)).values
    neg_items = interected_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values

    return (
        torch.LongTensor(list(users)).to(device),
        torch.LongTensor(list(pos_items)).to(device) + n_usr,
        torch.LongTensor(list(neg_items)).to(device) + n_usr
    )

data_loader(train_df, 16, n_users, n_items)

u_t = torch.LongTensor(train_df.user_id_idx)
i_t = torch.LongTensor(train_df.item_id_idx) + n_users

train_edge_index = torch.stack((
  torch.cat([u_t, i_t]),
  torch.cat([i_t, u_t])
)).to(device)
train_edge_index

train_edge_index[:,-1], train_edge_index[:, 0]

train_edge_index[:, len(train)-1], train_edge_index[:, len(train)]

class LightGCNConv(MessagePassing):
  def __init__(self, **kwargs):
    super().__init__(aggr='add')

  def forward(self, x, edge_index):
    # Compute normalization
    from_, to_ = edge_index
    deg = degree(to_, x.size(0), dtype=x.dtype)
    deg_inv_sqrt = deg.pow(-0.5)
    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
    norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]

    # Start propagating messages (no update after aggregation)
    return self.propagate(edge_index, x=x, norm=norm)

  def message(self, x_j, norm):
    return norm.view(-1, 1) * x_j

test_x = torch.Tensor(np.eye(5))
test_edge_index = torch.LongTensor(np.array([
  [0, 0, 1, 1, 2, 3, 3, 4],
  [2, 3, 3, 4, 0, 0, 1, 1]
]))

LightGCNConv()(test_x, test_edge_index)

class RecSysGNN(nn.Module):
  def __init__(
      self,
      latent_dim,
      num_layers,
      num_users,
      num_items,
      model
  ):
    super(RecSysGNN, self).__init__()

    assert (model == 'LightGCN'), \
        'Model must be LightGCN'
    self.model = model
    self.embedding = nn.Embedding(num_users + num_items, latent_dim)

    if self.model == 'LightGCN':
      self.convs = nn.ModuleList(LightGCNConv() for _ in range(num_layers))

    self.init_parameters()


  def init_parameters(self):
    if self.model == 'LightGCN':
      # Authors of LightGCN report higher results with normal initialization
      nn.init.normal_(self.embedding.weight, std=0.1)


  def forward(self, edge_index):
    emb0 = self.embedding.weight
    embs = [emb0]

    emb = emb0
    for conv in self.convs:
      emb = conv(x=emb, edge_index=edge_index)
      embs.append(emb)

    out = (
      torch.cat(embs, dim=-1) if self.model == 'NGCF'
      else torch.mean(torch.stack(embs, dim=0), dim=0)
    )

    return emb0, out


  def encode_minibatch(self, users, pos_items, neg_items, edge_index):
    emb0, out = self(edge_index)
    return (
        out[users],
        out[pos_items],
        out[neg_items],
        emb0[users],
        emb0[pos_items],
        emb0[neg_items]
    )

def compute_bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb0,  pos_emb0, neg_emb0):
  # compute loss from initial embeddings, used for regulization
  reg_loss = (1 / 2) * (
    user_emb0.norm().pow(2) +
    pos_emb0.norm().pow(2)  +
    neg_emb0.norm().pow(2)
  ) / float(len(users))

  # compute BPR loss from user, positive item, and negative item embeddings
  pos_scores = torch.mul(users_emb, pos_emb).sum(dim=1)
  neg_scores = torch.mul(users_emb, neg_emb).sum(dim=1)

  bpr_loss = torch.mean(F.softplus(neg_scores - pos_scores))

  return bpr_loss, reg_loss

def get_metrics(user_Embed_wts, item_Embed_wts, n_users, n_items, train_data, test_data, K):
    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())
    # compute the score of all user-item pairs
    relevance_score = torch.matmul(user_Embed_wts, torch.transpose(item_Embed_wts, 0, 1))

    # create dense tensor of all user-item interactions
    i = torch.stack((
        torch.LongTensor(train_df['user_id_idx'].values),
        torch.LongTensor(train_df['item_id_idx'].values)
    ))
    indices = torch.LongTensor(i).to(device)
    shape = (n_users, n_items)
    interactions_t = torch.sparse_coo_tensor(indices, torch.ones(len(train_df), dtype=torch.float32).to(device), shape, device=device)

    # convert interactions_t to dense tensor
    interactions_dense = interactions_t.to_dense()

    # mask out training user-item interactions from metric computation
    relevance_score = relevance_score * (1 - interactions_dense)

    # compute top scoring items for each user
    topk_relevance_indices = torch.topk(relevance_score, K).indices
    topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.cpu().numpy(), columns=['top_indx_'+str(x+1) for x in range(K)])
    topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index
    topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()
    topk_relevance_indices_df = topk_relevance_indices_df[['user_ID','top_rlvnt_itm']]

    # measure overlap between recommended (top-scoring) and held-out user-item
    # interactions
    test_interacted_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()
    metrics_df = pd.merge(test_interacted_items, topk_relevance_indices_df, how='left', left_on='user_id_idx', right_on=['user_ID'])
    metrics_df['intrsctn_itm'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.top_rlvnt_itm)]

    metrics_df['recall'] = metrics_df.apply(lambda x: len(x['intrsctn_itm'])/len(x['item_id_idx']), axis=1)
    metrics_df['precision'] = metrics_df.apply(lambda x: len(x['intrsctn_itm'])/K, axis=1)

    return metrics_df['recall'].mean(), metrics_df['precision'].mean()

latent_dim = 64
n_layers = 3

EPOCHS = 50
BATCH_SIZE = 1024
DECAY = 0.0001
LR = 0.005
K = 20

def train_and_eval(model, optimizer, train_df):
  loss_list_epoch = []
  bpr_loss_list_epoch = []
  reg_loss_list_epoch = []

  recall_list = []
  precision_list = []

  for epoch in tqdm(range(EPOCHS)):
      n_batch = int(len(train)/BATCH_SIZE)

      final_loss_list = []
      bpr_loss_list = []
      reg_loss_list = []

      model.train()
      for batch_idx in range(n_batch):

          optimizer.zero_grad()

          users, pos_items, neg_items = data_loader(train_df, BATCH_SIZE, n_users, n_items)
          users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = model.encode_minibatch(users, pos_items, neg_items, train_edge_index)

          bpr_loss, reg_loss = compute_bpr_loss(
            users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0
          )
          reg_loss = DECAY * reg_loss
          final_loss = bpr_loss + reg_loss

          final_loss.backward()
          optimizer.step()

          final_loss_list.append(final_loss.item())
          bpr_loss_list.append(bpr_loss.item())
          reg_loss_list.append(reg_loss.item())

      model.eval()
      with torch.no_grad():
          _, out = model(train_edge_index)
          final_user_Embed, final_item_Embed = torch.split(out, (n_users, n_items))
          test_topK_recall,  test_topK_precision = get_metrics(
            final_user_Embed, final_item_Embed, n_users, n_items, train_df, test_df, K
          )

      loss_list_epoch.append(round(np.mean(final_loss_list),4))
      bpr_loss_list_epoch.append(round(np.mean(bpr_loss_list),4))
      reg_loss_list_epoch.append(round(np.mean(reg_loss_list),4))

      recall_list.append(round(test_topK_recall,4))
      precision_list.append(round(test_topK_precision,4))

  return (
    loss_list_epoch,
    bpr_loss_list_epoch,
    reg_loss_list_epoch,
    recall_list,
    precision_list
  )

lightgcn = RecSysGNN(
  latent_dim=latent_dim,
  num_layers=n_layers,
  num_users=n_users,
  num_items=n_items,
  model='LightGCN'
)
lightgcn.to(device)

optimizer = torch.optim.Adam(lightgcn.parameters(), lr=LR)


light_loss, light_bpr, light_reg, light_recall, light_precision = train_and_eval(lightgcn, optimizer, train_df)

# Define a dictionary containing model information
model_info = {
    'model_architecture': 'LightGCN',
    'latent_dim': latent_dim,
    'num_layers': n_layers,
    'num_users': n_users,
    'num_items': n_items,
    'training_params': {
        'epochs': EPOCHS,
        'batch_size': BATCH_SIZE,
        'decay': DECAY,
        'learning_rate': LR,
        'top_k': K
    }
}

# Save model and model information
torch.save({
    'model_state_dict': lightgcn.state_dict(),
    'model_info': model_info
}, 'lightgcn1_model.pth')

epoch_list = [(i+1) for i in range(EPOCHS)]

plt.plot(epoch_list, light_loss, label='Total Training Loss')
plt.plot(epoch_list, light_bpr, label='BPR Training Loss')
plt.plot(epoch_list, light_reg, label='Reg Training Loss')

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.plot(epoch_list, light_recall, label='Recall')
plt.plot(epoch_list, light_precision, label='Precision')
plt.xlabel('Epoch')
plt.ylabel('Metrics')
plt.legend()

max(light_precision), max(light_recall)

import pandas as pd

# Load the movies DataFrame from movies.csv
movies_df = pd.read_csv('C:\\Users\\HP\\Documents\\Major_Project_Files\\frontend\\ml-latest-small\\ml-latest-small\\movies.csv')

# Extract unique genres
unique_genres = set('|'.join(movies_df['genres']).split('|'))

# Remove "(no genres listed)" from unique genres
unique_genres.discard("(no genres listed)")

# Convert unique genres to list
available_genres = list(unique_genres)


import pandas as pd
import random

def load_movies_data(file_path):
    movies_df = pd.read_csv(file_path)
    return movies_df

def user_preferences(available_genres):
    print("Available Genres:")
    for idx, genre in enumerate(available_genres, start=1):
        print(f"{idx}. {genre}")

    selected_genres_input = input("Enter the numbers of the genres you prefer (separated by commas): ")
    selected_genre_indices = [int(idx) for idx in selected_genres_input.split(',')]
    selected_genres = [available_genres[idx - 1] for idx in selected_genre_indices]
    return selected_genres

def get_top_recommendations(user_id, selected_genres, model, movies_df, le_item, n_items, top_n=10):
    filtered_movies = movies_df[movies_df['genres'].apply(lambda x: any(genre in x for genre in selected_genres))]

    with torch.no_grad():
        _, all_item_embeddings = model(train_edge_index)

    user_embedding = model.embedding(torch.tensor([user_id], dtype=torch.int64).to(device))
    similarities = F.cosine_similarity(user_embedding, all_item_embeddings[n_users:], dim=-1)
    top_indices = torch.argsort(similarities, descending=True)[:n_items]
    recommended_movie_ids = le_item.inverse_transform(top_indices.cpu().numpy())
    recommended_movie_ids = [movie_id for movie_id in recommended_movie_ids if movie_id in movies_df.index]
    recommendations = movies_df.loc[recommended_movie_ids]
    recommendations['matching_genres'] = recommendations['genres'].apply(lambda x: sum(genre in x for genre in selected_genres))
    recommendations = recommendations.sort_values(by='matching_genres', ascending=False)
    top_recommendations = recommendations.head(top_n)
    top_recommendations_copy = top_recommendations.copy()
    top_recommendations_copy.reset_index(drop=True, inplace=True)
    top_recommendations_copy['movieId'] = top_recommendations_copy.index
    return top_recommendations_copy[['movieId', 'title']]

def main():
    # Load movies data
    movies_df = load_movies_data('C:\\Users\\HP\\Documents\\Major_Project_Files\\frontend\\ml-latest-small\\ml-latest-small\\movies.csv')

    # Get user preferences
    selected_genres = user_preferences(list(set('|'.join(movies_df['genres']).split('|')) - {"(no genres listed)"}))

    # Get top recommendations
    random_user_id = random.choice(train_user_ids)
    top_recommendations = get_top_recommendations(random_user_id, selected_genres, lightgcn, movies_df, le_item, n_items)

    # Load ratings data
    ratings_df = pd.read_csv('C:\\Users\\HP\\Documents\\Major_Project_Files\\frontend\\ml-latest-small\\ml-latest-small\\ratings.csv')

    # Filter and sort recommendations based on ratings and year
    recommended_ratings = ratings_df[ratings_df['movieId'].isin(top_recommendations['movieId'])]
    merged_data = pd.merge(top_recommendations, recommended_ratings, on='movieId', how='left')
    average_ratings = merged_data.groupby(['movieId', 'title'], as_index=False)['rating'].mean()
    sorted_recommendations = average_ratings.sort_values(by='rating', ascending=False)

    # Display sorted recommendations
    sorted_recommendations['year'] = sorted_recommendations['title'].str.extract(r'\((\d{4})\)')
    sorted_recommendations['year'] = pd.to_numeric(sorted_recommendations['year'])
    sorted_recommendations = sorted_recommendations.sort_values(by=['rating', 'year'], ascending=[False, False])
    print(sorted_recommendations[['title']].to_string(index=False))

if __name__ == "__main__":
    main()
