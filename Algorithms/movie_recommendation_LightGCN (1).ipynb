{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Movie Recommendation with LightGCNâœ¨**\n",
        "\n",
        "\n",
        "This notebook walks you through building a movie recommendation system using LightGCN. You can read more about Graph Neural Networks (GNNs), LightGCN and movie recommendation in [this Medium tutorial](https://medium.com/@quinn.wang/eb6d112f1e8).\n",
        "\n",
        "By Qinchen Wang, Xiaoli Yang, Feiyang (Kathy) Yu as part of the Stanford [CS224W: Machine Learning with Graphs](http://web.stanford.edu/class/cs224w/) course project.\n",
        "\n",
        "Without further ado, let's get started!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mu50kKA5_Dur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "We recommend using a hardware accelerator for the notebook. You can set a GPU via \"Runtime\" --> \"Change runtime type\"."
      ],
      "metadata": {
        "id": "J_SwBpzH_SLQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jhucurizRlt",
        "outputId": "9c6c7b34-e2c7-4830-aa34-d8fcd7518570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorly in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorly) (1.19.5)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.7/dist-packages (from tensorly) (1.3.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tensorly) (1.4.1)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorly\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import os.path as osp\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "import random\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.display.max_rows = 10\n",
        "from sklearn import metrics\n",
        "from tensorly import decomposition\n",
        "\n",
        "import torch\n",
        "from torch.functional import tensordot\n",
        "from torch import nn, optim, Tensor\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Dataset, Data, download_url, extract_zip\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.typing import Adj"
      ],
      "metadata": {
        "id": "ue8ts-4k0Wvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch has version {torch.__version__}\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYW0MwN31fdW",
        "outputId": "5a6fff13-7ea1-43fe-eb22-68962914b762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 1.10.0+cu111\n",
            "Torch version: 1.10.0+cu111\n",
            "Cuda available: True\n",
            "Torch geometric version: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations\n",
        "\n",
        "Configure the model and training process. These parameters will make more sense as you move along."
      ],
      "metadata": {
        "id": "Wl2uLQvH_Vpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_threshold = 3  #@param {type: \"integer\"}: Ratings equal to or greater than 3 are positive items.\n",
        "\n",
        "config_dict = {\n",
        "    \"num_samples_per_user\": 500,\n",
        "    \"num_users\": 200,\n",
        "\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 0.001,\n",
        "    \"weight_decay\": 0.1,\n",
        "\n",
        "    \"embedding_size\": 64,\n",
        "    \"num_layers\": 5,\n",
        "    \"K\": 10,\n",
        "    \"mf_rank\": 8,\n",
        "\n",
        "    \"minibatch_per_print\": 100,\n",
        "    \"epochs_per_print\": 1,\n",
        "\n",
        "    \"val_frac\": 0.2,\n",
        "    \"test_frac\": 0.1,\n",
        "\n",
        "    \"model_name\": \"model.pth\"\n",
        "}"
      ],
      "metadata": {
        "id": "_nAUZ8LY0wyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "A great publicly available dataset for training movie recommenders is the MovieLens 1M dataset. The MovieLens 1M dataset consists of 1 million movie ratings of score 1 to 5, from 6000 users and 4000 movies."
      ],
      "metadata": {
        "id": "fpZBeK9B_Z9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\""
      ],
      "metadata": {
        "id": "8aFE4Jbg14IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trans_ml(dat, thres):\n",
        "    \"\"\"\n",
        "    Transform function that assign non-negative entries >= thres 1, and non-\n",
        "    negative entries <= thres 0. Keep other entries the same.\n",
        "    \"\"\"\n",
        "    thres = thres[0]\n",
        "    matrix = dat['edge_index']\n",
        "    matrix[(matrix < thres) & (matrix > -1)] = 0\n",
        "    matrix[(matrix >= thres)] = 1\n",
        "    dat['edge_index'] = matrix\n",
        "    return dat\n",
        "\n",
        "\n",
        "class MovieLens(Dataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None,\n",
        "            transform_args=None, pre_transform_args=None):\n",
        "        \"\"\"\n",
        "        root = where the dataset should be stored. This folder is split\n",
        "        into raw_dir (downloaded dataset) and processed_dir (process data).\n",
        "        \"\"\"\n",
        "        super(MovieLens, self).__init__(root, transform, pre_transform)\n",
        "        self.transform = transform\n",
        "        self.pre_transform = pre_transform\n",
        "        self.transform_args = transform_args\n",
        "        self.pre_transform_args = pre_transform_args\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return \"ml-1m.zip\"\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [\"data_movielens.pt\"]\n",
        "\n",
        "    def download(self):\n",
        "        # Download to `self.raw_dir`.\n",
        "        download_url(DATA_PATH, self.raw_dir)\n",
        "\n",
        "    def _load(self):\n",
        "        print(self.raw_dir)\n",
        "        # extract_zip(self.raw_paths[0], self.raw_dir)\n",
        "        with zipfile.ZipFile(self.raw_paths[0], 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.raw_dir)\n",
        "        unames = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
        "        users = pd.read_table(self.raw_dir+'/ml-1m/users.dat',\n",
        "                              sep='::', header=None, names=unames,\n",
        "                              engine='python', encoding='latin-1')\n",
        "        rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
        "        ratings = pd.read_table(self.raw_dir+'/ml-1m/ratings.dat', sep='::',\n",
        "                                header=None, names=rnames, engine='python',\n",
        "                                encoding='latin-1')\n",
        "        mnames = ['movie_id', 'title', 'genres']\n",
        "        movies = pd.read_table(self.raw_dir+'/ml-1m/movies.dat', sep='::',\n",
        "                               header=None, names=mnames, engine='python',\n",
        "                               encoding='latin-1')\n",
        "        dat = pd.merge(pd.merge(ratings, users), movies)\n",
        "\n",
        "        return users, ratings, movies, dat\n",
        "\n",
        "    def process(self):\n",
        "        print('run process')\n",
        "        # load information from file\n",
        "        users, ratings, movies, dat = self._load()\n",
        "\n",
        "        users = users['user_id']\n",
        "        movies = movies['movie_id']\n",
        "\n",
        "        num_users = config_dict[\"num_users\"]\n",
        "        if num_users != -1:\n",
        "            users = users[:num_users]\n",
        "\n",
        "        user_ids = range(len(users))\n",
        "        movie_ids = range(len(movies))\n",
        "\n",
        "        user_to_id = dict(zip(users, user_ids))\n",
        "        movie_to_id = dict(zip(movies, movie_ids))\n",
        "\n",
        "        # get adjacency info\n",
        "        self.num_user = users.shape[0]\n",
        "        self.num_item = movies.shape[0]\n",
        "\n",
        "        # initialize the adjacency matrix\n",
        "        rat = torch.zeros(self.num_user, self.num_item)\n",
        "\n",
        "        for index, row in ratings.iterrows():\n",
        "            user, movie, rating = row[:3]\n",
        "            if num_users != -1:\n",
        "                if user not in user_to_id: break\n",
        "            # create ratings matrix where (i, j) entry represents the ratings\n",
        "            # of movie j given by user i.\n",
        "            rat[user_to_id[user], movie_to_id[movie]] = rating\n",
        "\n",
        "        # create Data object\n",
        "        data = Data(edge_index = rat,\n",
        "                    raw_edge_index = rat.clone(),\n",
        "                    data = ratings,\n",
        "                    users = users,\n",
        "                    items = movies)\n",
        "\n",
        "        # apply any pre-transformation\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data, self.pre_transform_args)\n",
        "\n",
        "        # apply any post_transformation\n",
        "        # if self.transform is not None:\n",
        "        #     # data = self.transform(data, self.transform_args)\n",
        "        data = self.transform(data, [rating_threshold])\n",
        "\n",
        "        # save the processed data into .pt file\n",
        "        torch.save(data, osp.join(self.processed_dir, f'data_movielens.pt'))\n",
        "        print('process finished')\n",
        "\n",
        "    def len(self):\n",
        "        \"\"\"\n",
        "        return the number of examples in your graph\n",
        "        \"\"\"\n",
        "        # TODO: how to define number of examples\n",
        "        return\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        The logic to load a single graph\n",
        "        \"\"\"\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_movielens.pt'))\n",
        "        return data\n",
        "\n",
        "    def train_val_test_split(self, val_frac=0.2, test_frac=0.1):\n",
        "        \"\"\"\n",
        "        Return two mask matrices (M, N) that represents edges present in the\n",
        "        train and validation set\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.num_user, self.num_item\n",
        "        except AttributeError:\n",
        "            data = self.get()\n",
        "            self.num_user = len(data[\"users\"].unique())\n",
        "            self.num_item = len(data[\"items\"].unique())\n",
        "        # get number of edges masked for training and validation\n",
        "        num_train_replaced = \\\n",
        "            round((test_frac+val_frac)*self.num_user*self.num_item)\n",
        "        num_val_show = round(val_frac*self.num_user*self.num_item)\n",
        "\n",
        "        # edges masked during training\n",
        "        indices_user = np.random.randint(0, self.num_user, num_train_replaced)\n",
        "        indices_item = np.random.randint(0, self.num_item, num_train_replaced)\n",
        "\n",
        "        # sample part of edges from training stage to be unmasked during\n",
        "        # validation\n",
        "        indices_val_user = np.random.choice(indices_user, num_val_show)\n",
        "        indices_val_item = np.random.choice(indices_item, num_val_show)\n",
        "\n",
        "        train_mask = torch.ones(self.num_user, self.num_item)\n",
        "        train_mask[indices_user, indices_item] = 0\n",
        "\n",
        "        val_mask = train_mask.clone()\n",
        "        val_mask[indices_val_user, indices_val_item] = 1\n",
        "\n",
        "        test_mask = torch.ones_like(train_mask)\n",
        "\n",
        "        return train_mask, val_mask, test_mask"
      ],
      "metadata": {
        "id": "IaZK6fwHzyd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGCN implementation\n",
        "\n",
        "Now let's dive into the model!"
      ],
      "metadata": {
        "id": "Fo-HN_lZ_8w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGCN neiborhood aggregation layer\n",
        "\n",
        "Starting with the initial embeddings $E^{(0)}$ and the bipartite graph, we iterate over each node to perform neighborhood aggregation. Note that LightGCN uses **a simple weighted sum aggregator** and **avoids the heavy-lifting feature transformation and nonlinear activation**.\n",
        "\n",
        "Within each layer, for each user in the graph, we compute its updated embedding as the weighted sum of embeddings from all its neighboring items (movies) following the formula below:\n",
        "$$ \\textbf{e}_u^{(k+1)} = \\sum_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|} \\sqrt{|N_i|}} \\textbf{e}_i^{(k)} $$\n",
        "where $ \\textbf{e}_u^{(k)} $ and $ \\textbf{e}_i^{(k)} $ are the user and item (movie) node embeddings at the k-th layer. $ |N_u| $ and $ |N_i| $ are the user and item nodesâ€™ number of neighbors.\n",
        "\n",
        "Similarly, for each item, the updated embedding is computed using weighted sum of its neighboring users:\n",
        "$$ \\textbf{e}_i^{(k+1)} = \\sum_{i \\in N_i} \\frac{1}{\\sqrt{|N_i|} \\sqrt{|N_u|}} \\textbf{e}_u^{(k)} $$"
      ],
      "metadata": {
        "id": "qEhZbrpVBeFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCNConv(MessagePassing):\n",
        "    r\"\"\"The neighbor aggregation operator from the `\"LightGCN: Simplifying and\n",
        "    Powering Graph Convolution Network for Recommendation\"\n",
        "    <https://arxiv.org/abs/2002.02126#>`_ paper\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
        "            the size from the first input(s) to the forward method.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        num_users (int): Number of users for recommendation.\n",
        "        num_items (int): Number of items to recommend.\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 num_users: int, num_items: int, **kwargs):\n",
        "        super(LightGCNConv, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        pass  # There are no layer parameters to learn.\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj) -> Tensor:\n",
        "        \"\"\"Performs neighborhood aggregation for user/item embeddings.\"\"\"\n",
        "        user_item = \\\n",
        "                torch.zeros(self.num_users, self.num_items, device=x.device)\n",
        "        user_item[edge_index[:, 0], edge_index[:, 1]] = 1\n",
        "        user_neighbor_counts = torch.sum(user_item, axis=1)\n",
        "        item_neightbor_counts = torch.sum(user_item, axis=0)\n",
        "        # Compute weight for aggregation: 1 / sqrt(N_u * N_i)\n",
        "        weights = user_item / torch.sqrt(\n",
        "                user_neighbor_counts.repeat(self.num_items, 1).T \\\n",
        "                * item_neightbor_counts.repeat(self.num_users, 1))\n",
        "        weights = torch.nan_to_num(weights, nan=0)\n",
        "        out = torch.concat((weights.T @ x[:self.num_users],\n",
        "                            weights @ x[self.num_users:]), 0)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)\n"
      ],
      "metadata": {
        "id": "WLlXODVwzkJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGCN model\n",
        "\n",
        "At layer combination, instead of taking the embedding of the final layer, LightGCN computes **a weighted sum of the embeddings at different layers**:\n",
        "$$ \\textbf{e}_u = \\sum_{k=0}^K \\alpha_k \\textbf{e}_u^{(k)} $$\n",
        "$$ \\textbf{e}_i = \\sum_{k=0}^K \\alpha_k \\textbf{e}_i^{(k)} $$\n",
        "with $ \\alpha \\ge 0 $. Here, alpha values can either be learned as network parameters, or set as empirical hyperparameters. It has been found that $ \\alpha = \\frac{1}{K + 1} $ works well.\n",
        "\n",
        "LightGCN predicts based on the inner product of the final user and item (movie) embeddings:\n",
        "$$ \\hat{y}_{ui} = \\textbf{e}_u^T \\textbf{e}_i $$\n",
        "This inner product measures the similarity between the user and movie, therefore allowing us to understand how likely it is for the user to like the movie."
      ],
      "metadata": {
        "id": "QWt5WAIjBiQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 config: dict,\n",
        "                 device=None,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_users  = config[\"n_users\"]\n",
        "        self.num_items  = config[\"m_items\"]\n",
        "        self.embedding_size = config[\"embedding_size\"]\n",
        "        self.in_channels = self.embedding_size\n",
        "        self.out_channels = self.embedding_size\n",
        "        self.num_layers = config[\"num_layers\"]\n",
        "\n",
        "        # 0-th layer embedding.\n",
        "        self.embedding_user_item = torch.nn.Embedding(\n",
        "            num_embeddings=self.num_users + self.num_items,\n",
        "            embedding_dim=self.embedding_size)\n",
        "        self.alpha = None\n",
        "\n",
        "        # random normal init seems to be a better choice when lightGCN actually\n",
        "        # don't use any non-linear activation function\n",
        "        nn.init.normal_(self.embedding_user_item.weight, std=0.1)\n",
        "        print('use NORMAL distribution initilizer')\n",
        "\n",
        "        self.f = nn.Sigmoid()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(LightGCNConv(\n",
        "                self.embedding_size, self.embedding_size,\n",
        "                num_users=self.num_users, num_items=self.num_items, **kwargs))\n",
        "\n",
        "        for _ in range(1, self.num_layers):\n",
        "            self.convs.append(\n",
        "                LightGCNConv(\n",
        "                        self.embedding_size, self.embedding_size,\n",
        "                        num_users=self.num_users, num_items=self.num_items,\n",
        "                        **kwargs))\n",
        "\n",
        "        self.device = None\n",
        "        if device is not None:\n",
        "            self.convs.to(device)\n",
        "            self.device = device\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:\n",
        "        xs: List[Tensor] = []\n",
        "\n",
        "        edge_index = torch.nonzero(edge_index)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index, *args, **kwargs)\n",
        "            if self.device is not None:\n",
        "                x = x.to(self.device)\n",
        "            xs.append(x)\n",
        "        xs = torch.stack(xs)\n",
        "\n",
        "        self.alpha = 1 / (1 + self.num_layers) * torch.ones(xs.shape)\n",
        "        if self.device is not None:\n",
        "            self.alpha = self.alpha.to(self.device)\n",
        "            xs = xs.to(self.device)\n",
        "        x = (xs * self.alpha).sum(dim=0)  # Sum along K layers.\n",
        "        return x\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, num_layers={self.num_layers})')"
      ],
      "metadata": {
        "id": "wz80KhdizkOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions\n",
        "\n",
        "The utility functions allow us to retrieve embeddings and compute user-item similarities. These will become userful later on."
      ],
      "metadata": {
        "id": "kkRJgY39BkLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getUsersRating(model, users, data):\n",
        "    \"\"\" Get the embedding of users\n",
        "    INPUT:\n",
        "        model: the LightGCN model you are training on\n",
        "        users: this is the user index (note: use 0-indexed and not user number,\n",
        "            which is 1-indexed)\n",
        "        data: the entire data, used to fetch all users and all items\n",
        "    \"\"\"\n",
        "    all_users_items = model(model.embedding_user_item.weight.clone(),\n",
        "                            data[\"edge_index\"])\n",
        "    all_users = all_users_items[:len(data[\"users\"])]\n",
        "    items_emb = all_users_items[len(data[\"users\"]):]\n",
        "    users_emb = all_users[users.long()]\n",
        "    rating = model.f(torch.matmul(users_emb, items_emb.t()))\n",
        "    return rating\n",
        "\n",
        "def getEmbedding(model, users, pos, neg, data, mask):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        model: the LightGCN model you are training on\n",
        "        users: this is the user index (note: use 0-indexed and not user number,\n",
        "            which is 1-indexed)\n",
        "        pos: positive index corresponding to an item that the user like\n",
        "        neg: negative index corresponding to an item that the user doesn't like\n",
        "        data: the entire data, used to fetch all users and all items\n",
        "        mask: Masking matrix indicating edges present in the current\n",
        "            train / validation / test set.\n",
        "    \"\"\"\n",
        "    # assuming we always search for users and items by their indices (instead of\n",
        "    # user/item number)\n",
        "    all_users_items = model(model.embedding_user_item.weight.clone(),\n",
        "                            data[\"edge_index\"] * mask)\n",
        "    all_users = all_users_items[:len(data[\"users\"])]\n",
        "    all_items = all_users_items[len(data[\"users\"]):]\n",
        "    users_emb = all_users[users]\n",
        "    pos_emb = all_items[pos]\n",
        "    neg_emb = all_items[neg]\n",
        "    n_user = len(data[\"users\"])\n",
        "    users_emb_ego = model.embedding_user_item(users)\n",
        "    # offset the index to fetch embedding from user_item\n",
        "    pos_emb_ego = model.embedding_user_item(pos + n_user)\n",
        "    neg_emb_ego = model.embedding_user_item(neg + n_user)\n",
        "    return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego"
      ],
      "metadata": {
        "id": "yCHmRyqw_s0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Personalized Ranking loss (BPR loss)\n",
        "\n",
        "To train the LightGCN model, we need an objective function that aligns with our goal for movie recommendation. We use the Bayesian Personalized Ranking (BPR) loss, which encourages observed user-item predictions to have increasingly higher values than unobserved ones, along with $ L_2 $ regularization:\n",
        "$$ L_{BPR} = - \\sum_{u=1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj}) + \\lambda ||\\textbf{E}^{(0)} ||^2 $$\n",
        "where $ \\textbf{E}^{(0)} $ is a matrix with column vectors being the 0-th layer embeddings to learn."
      ],
      "metadata": {
        "id": "K7ct4L3DBnFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(model, users, pos, neg, data, mask):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        model: the LightGCN model you are training on\n",
        "        users: this is the user index (note: use 0-indexed and not user number,\n",
        "            which is 1-indexed)\n",
        "        pos: positive index corresponding to an item that the user like\n",
        "            (0-indexed, note to index items starting from 0)\n",
        "        neg: negative index corresponding to an item that the user doesn't like\n",
        "        data: the entire data, used to fetch all users and all items\n",
        "        mask: Masking matrix indicating edges present in the current\n",
        "            train / validation / test set.\n",
        "    OUTPUT:\n",
        "        loss, reg_loss\n",
        "    \"\"\"\n",
        "    # assuming we always sample the same number of positive and negative sample\n",
        "    # per user\n",
        "    assert len(users) == len(pos) and len(users) == len(neg)\n",
        "    (users_emb, pos_emb, neg_emb,\n",
        "    userEmb0,  posEmb0, negEmb0) = getEmbedding(model, users.long(), pos.long(),\n",
        "                                                neg.long(), data, mask)\n",
        "    reg_loss = (1/2)*(userEmb0.norm(2).pow(2) +\n",
        "                        posEmb0.norm(2).pow(2)  +\n",
        "                        negEmb0.norm(2).pow(2))/float(len(users))\n",
        "    pos_scores = torch.mul(users_emb, pos_emb)\n",
        "    pos_scores = torch.sum(pos_scores, dim=1)\n",
        "    neg_scores = torch.mul(users_emb, neg_emb)\n",
        "    neg_scores = torch.sum(neg_scores, dim=1)\n",
        "\n",
        "    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
        "\n",
        "    return loss, reg_loss"
      ],
      "metadata": {
        "id": "dZ62Sk46_uxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Personalized top K precision and recall\n",
        "\n",
        "To evaluate training progress and model performance, we compute the **top K precision and recall** scores. Specifically, for each user, we rank movie items in order of decreasing similarity and choose the best K to recommend. Then, we compute the precision and recall of those K recommendations against ground truth items that the user likes and dislikes."
      ],
      "metadata": {
        "id": "eni-XlbOBydp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def personalized_topk(pred, K, user_indices, edge_index):\n",
        "    \"\"\"Computes TopK precision and recall.\n",
        "\n",
        "    Args:\n",
        "        pred: Predicted similarities between user and item.\n",
        "        K: Number of items to rank.\n",
        "        user_indices: Indices of users for each prediction in `pred`.\n",
        "        edge_index: User and item connection matrix.\n",
        "\n",
        "    Returns:\n",
        "        Average Top K precision and recall for users in `user_indices`.\n",
        "    \"\"\"\n",
        "    per_user_preds = collections.defaultdict(list)\n",
        "    for index, user in enumerate(user_indices):\n",
        "        per_user_preds[user.item()].append(pred[index].item())\n",
        "    precisions = 0.0\n",
        "    recalls = 0.0\n",
        "    for user, preds in per_user_preds.items():\n",
        "        while len(preds) < K:\n",
        "            preds.append(random.choice(range(edge_index.shape[1])))\n",
        "        top_ratings, top_items = torch.topk(torch.tensor(preds), K)\n",
        "        correct_preds = edge_index[user, top_items].sum().item()\n",
        "        total_pos = edge_index[user].sum().item()\n",
        "        precisions += correct_preds / K\n",
        "        recalls += correct_preds / total_pos if total_pos != 0 else 0\n",
        "    num_users = len(user_indices.unique())\n",
        "    return precisions / num_users, recalls / num_users"
      ],
      "metadata": {
        "id": "VjHb_Cr0_0gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training, validation and testing\n",
        "\n",
        "Now, let's train our LightGCN model, and run it on the validation and test sets."
      ],
      "metadata": {
        "id": "8VVAYXeFANXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "\n",
        "For each user, we randomly sample $n$ positive-negative movie examples and add them to the training, validation or test set. $n$ is a parameter that we can specify and tune."
      ],
      "metadata": {
        "id": "Z4VffTxqAkMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _sample_pos_neg(data, mask, num_samples_per_user):\n",
        "    \"\"\"Samples (user, positive item, negative item) tuples per user.\n",
        "\n",
        "    If a user does not have a postive (negative) item, we choose an item\n",
        "    with unknown liking (an item without raw rating data).\n",
        "\n",
        "    Args:\n",
        "        data: Dataset object containing edge_index and raw ratings matrix.\n",
        "        mask: Masking matrix indicating edges present in the current\n",
        "            train / validation / test set.\n",
        "        num_samples_per_user: Number of samples to generate for each user.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor object of (user, positive item, negative item) samples.\n",
        "    \"\"\"\n",
        "    print(\"=====Starting to sample=====\")\n",
        "    start = time.time()\n",
        "    samples = []\n",
        "    all_items = set(range(len(data[\"items\"])))\n",
        "    for user_index, user in enumerate(data[\"users\"]):\n",
        "        pos_items = set(\n",
        "            torch.nonzero(data[\"edge_index\"][user_index])[:, 0].tolist())\n",
        "        unknown_items = all_items.difference(\n",
        "                set(\n",
        "                    torch.nonzero(\n",
        "                        data[\"raw_edge_index\"][user_index])[:, 0].tolist()))\n",
        "        neg_items = all_items.difference(\n",
        "            set(pos_items)).difference(set(unknown_items))\n",
        "        unmasked_items = set(torch.nonzero(mask[user_index])[:, 0].tolist())\n",
        "        if len(unknown_items.union(pos_items)) == 0 or \\\n",
        "                len(unknown_items.union(neg_items)) == 0:\n",
        "            continue\n",
        "        for _ in range(num_samples_per_user):\n",
        "            if len(pos_items.intersection(unmasked_items)) == 0:\n",
        "                pos_item_index = random.choice(\n",
        "                    list(unknown_items.intersection(unmasked_items)))\n",
        "            else:\n",
        "                pos_item_index = random.choice(\n",
        "                    list(pos_items.intersection(unmasked_items)))\n",
        "            if len(neg_items.intersection(unmasked_items)) == 0:\n",
        "                neg_item_index = random.choice(\n",
        "                    list(unknown_items.intersection(unmasked_items)))\n",
        "            else:\n",
        "                neg_item_index = random.choice(\n",
        "                    list(neg_items.intersection(unmasked_items)))\n",
        "            samples.append((user_index, pos_item_index, neg_item_index))\n",
        "    end = time.time()\n",
        "    print(f\"=====Sampling completed (took {end - start} seconds)=====\")\n",
        "    return torch.tensor(samples, dtype=torch.int32)\n",
        "\n",
        "def sample_pos_neg(data, train_mask, val_mask, test_mask, num_samples_per_user):\n",
        "    \"\"\"Samples (user, positive item, negative item) tuples per user.\n",
        "\n",
        "    If a user does not have a postive (negative) item, we choose an item\n",
        "    with unknown liking (an item without raw rating data).\n",
        "\n",
        "    Args:\n",
        "        data: Dataset object containing edge_index and raw ratings matrix.\n",
        "        train_mask: Masking matrix indicating edges present in train set.\n",
        "        val_mask: Masking matrix indicating edges present in validation set.\n",
        "        test_mask: Masking matrix indicating edges present in test set.\n",
        "        num_samples_per_user: Number of samples to generate for each user.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor object of (user, positive item, negative item) samples for\n",
        "        train, validation and test.\n",
        "    \"\"\"\n",
        "    train_samples = _sample_pos_neg(data, train_mask, num_samples_per_user)\n",
        "    val_samples = _sample_pos_neg(data, val_mask, num_samples_per_user)\n",
        "    test_samples = _sample_pos_neg(data, test_mask, num_samples_per_user)\n",
        "    return train_samples, val_samples, test_samples"
      ],
      "metadata": {
        "id": "qmAgY88nzzGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation\n",
        "\n",
        "Now, let's start training!"
      ],
      "metadata": {
        "id": "LwxJ6gDMAmSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = os.getcwd()\n",
        "movielens = MovieLens(root=root, transform=trans_ml)\n",
        "data = movielens.get()\n",
        "train_mask, val_mask, test_mask = \\\n",
        "        movielens.train_val_test_split(val_frac=config_dict[\"val_frac\"],\n",
        "                                       test_frac=config_dict[\"test_frac\"])\n",
        "\n",
        "n_users = len(data[\"users\"].unique())\n",
        "m_items = len(data[\"items\"].unique())\n",
        "print(f\"#Users: {n_users}\")\n",
        "print(f\"#Items: {m_items}\")\n",
        "\n",
        "model_config = {\n",
        "    \"n_users\": n_users,\n",
        "    \"m_items\": m_items,\n",
        "    \"embedding_size\": config_dict[\"embedding_size\"],\n",
        "    \"num_layers\": config_dict[\"num_layers\"],\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lightGCN = LightGCN(model_config, device=device)\n",
        "\n",
        "num_samples_per_user = config_dict[\"num_samples_per_user\"]\n",
        "epochs = config_dict[\"epochs\"]\n",
        "batch_size = config_dict[\"batch_size\"]\n",
        "lr = config_dict[\"lr\"]\n",
        "weight_decay = config_dict[\"weight_decay\"]\n",
        "\n",
        "K = config_dict[\"K\"]\n",
        "\n",
        "lightGCN.to(device)\n",
        "\n",
        "samples_train, samples_val, samples_test = \\\n",
        "        sample_pos_neg(data, train_mask, val_mask, test_mask,\n",
        "                       num_samples_per_user)\n",
        "\n",
        "samples_train=samples_train.to(device)\n",
        "samples_val=samples_val.to(device)\n",
        "samples_test=samples_test.to(device)\n",
        "train_mask=train_mask.to(device)\n",
        "val_mask=val_mask.to(device)\n",
        "test_mask=test_mask.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "print(f\"#Training samples: {len(samples_train)}\",\n",
        "      f\"#Validation samples: {len(samples_val)}\",\n",
        "      f\"#Test samples: {len(samples_test)}\")\n",
        "\n",
        "optimizer = optim.Adam(lightGCN.parameters(), lr=lr)\n",
        "print(\"Optimizer:\", optimizer)\n",
        "\n",
        "epochs_tracked = []\n",
        "train_topks = []\n",
        "val_topks = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"Training on the {} epoch\".format(epoch))\n",
        "    lightGCN.train()\n",
        "    loss_sum = 0\n",
        "    # Shuffle the order of rows.\n",
        "    samples_train = samples_train[torch.randperm(samples_train.size()[0])]\n",
        "    for batch_idx in range(math.ceil(len(samples_train) / batch_size)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        current_batch = \\\n",
        "            samples_train[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
        "        # Shuffle the order of rows.\n",
        "        current_batch = current_batch[torch.randperm(current_batch.size()[0])]\n",
        "        users = current_batch[:, 0:1]\n",
        "        pos = current_batch[:, 1:2]\n",
        "        neg = current_batch[:, 2:3]\n",
        "\n",
        "        loss, reg_loss = bpr_loss(lightGCN, users, pos, neg, data,\n",
        "                                  train_mask)\n",
        "        reg_loss = reg_loss * weight_decay\n",
        "        loss = loss + reg_loss\n",
        "        loss_sum += loss.detach()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % config_dict[\"minibatch_per_print\"] == 0:\n",
        "            all_users = torch.linspace(start=0,\n",
        "                                       end=n_users - 1, steps=n_users).long()\n",
        "            user_indices = current_batch[:, 0]\n",
        "            user_indices = user_indices.repeat(2).long()\n",
        "            item_indices = torch.cat(\n",
        "                (current_batch[:, 1], current_batch[:, 2])).long()\n",
        "            pred = getUsersRating(lightGCN,\n",
        "                                  all_users,\n",
        "                                  data)[user_indices, item_indices]\n",
        "            truth = data[\"edge_index\"][user_indices, item_indices]\n",
        "            topk_precision, topk_recall = \\\n",
        "                personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "\n",
        "            print(\"Training on epoch {} minibatch {}/{} completed\\n\".format(epoch, batch_idx+1,\n",
        "                                                                            math.ceil(len(samples_train) / batch_size)),\n",
        "                  \"bpr_loss on current minibatch is {}, and regularization loss is {}.\\n\".format(round(float(loss.detach().cpu()), 6),\n",
        "                                                                                                 round(float(reg_loss.detach().cpu()), 6)),\n",
        "                  \"Top K precision = {}, recall = {}.\".format(topk_precision, topk_recall))\n",
        "\n",
        "    if epoch % config_dict[\"epochs_per_print\"] == 0:\n",
        "        epochs_tracked.append(epoch)\n",
        "\n",
        "        # evaluation on both the trainisng and validation set\n",
        "        lightGCN.eval()\n",
        "        # predict on the training set\n",
        "        users = samples_train[:, 0:1]\n",
        "        user_indices = samples_train[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat(\n",
        "            (samples_train[:, 1], samples_train[:, 2])).long()\n",
        "        pred = getUsersRating(lightGCN,\n",
        "                              users[:,0],\n",
        "                              data)[user_indices, item_indices]\n",
        "        truth = data[\"edge_index\"][users.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        train_topk_precision, train_topk_recall = \\\n",
        "            personalized_topk(pred, K, user_indices, data[\"edge_index\"])\n",
        "        train_topks.append((train_topk_precision, train_topk_recall))\n",
        "\n",
        "        # predict on the validation set\n",
        "        users_val = samples_val[:, 0:1]\n",
        "        pos_val = samples_val[:, 1:2]\n",
        "        neg_val = samples_val[:, 2:3]\n",
        "\n",
        "        loss_val, reg_loss_val = bpr_loss(\n",
        "            lightGCN, users_val, pos_val, neg_val, data, val_mask)\n",
        "        reg_loss_val = reg_loss_val * weight_decay\n",
        "\n",
        "        # predict on the validation set\n",
        "        user_indices = samples_val[:, 0]\n",
        "        user_indices = user_indices.repeat(2).long()\n",
        "        item_indices = torch.cat((samples_val[:, 1], samples_val[:, 2])).long()\n",
        "        pred_val = getUsersRating(lightGCN,\n",
        "                                  users_val[:,0],\n",
        "                                  data)[user_indices, item_indices]\n",
        "        truth_val = data[\"edge_index\"][users_val.long()[:,0]]\\\n",
        "            [user_indices, item_indices]\n",
        "        val_topk_precision, val_topk_recall = \\\n",
        "            personalized_topk(pred_val, K, user_indices, data[\"edge_index\"])\n",
        "        val_topks.append((val_topk_precision, val_topk_recall))\n",
        "\n",
        "        print(\"\\nTraining on {} epoch completed.\\n\".format(epoch),\n",
        "              \"Average bpr_loss on train set is {} for the current epoch.\\n\".format(round(float(loss_sum/len(samples_train)), 6)),\n",
        "              \"Training top K precision = {}, recall = {}.\\n\".format(train_topk_precision, train_topk_recall),\n",
        "              \"Average bpr_loss on the validation set is {}, and regularization loss is {}.\\n\".format(round(float((loss_val+reg_loss_val)/len(samples_val)), 6),\n",
        "                                                                                                      round(float(reg_loss_val/len(samples_val)), 6)),\n",
        "              \"Validation top K precision = {}, recall = {}.\\n\".format(val_topk_precision, val_topk_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-NbgY1GzkR7",
        "outputId": "8cc0c651-0e5e-44bd-c496-038b0f54737e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Users: 200\n",
            "#Items: 3883\n",
            "use NORMAL distribution initilizer\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.6434710025787354 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.693074941635132 seconds)=====\n",
            "=====Starting to sample=====\n",
            "=====Sampling completed (took 2.8403525352478027 seconds)=====\n",
            "#Training samples: 100000 #Validation samples: 100000 #Test samples: 100000\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            "Training on the 0 epoch\n",
            "Training on epoch 0 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.788629, and regularization loss is 0.095482.\n",
            " Top K precision = 0.08369565217391305, recall = 0.006978145269310933.\n",
            "Training on epoch 0 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.739845, and regularization loss is 0.046698.\n",
            " Top K precision = 0.09479166666666665, recall = 0.007011445055315394.\n",
            "Training on epoch 0 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.717394, and regularization loss is 0.024247.\n",
            " Top K precision = 0.08461538461538459, recall = 0.007779529764432653.\n",
            "Training on epoch 0 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.706381, and regularization loss is 0.013233.\n",
            " Top K precision = 0.09081632653061221, recall = 0.006979089267249434.\n",
            "Training on epoch 0 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.70163, and regularization loss is 0.008483.\n",
            " Top K precision = 0.08775510204081631, recall = 0.007005625556417723.\n",
            "Training on epoch 0 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.698175, and regularization loss is 0.005028.\n",
            " Top K precision = 0.08762886597938142, recall = 0.007408940770021699.\n",
            "Training on epoch 0 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.697361, and regularization loss is 0.004213.\n",
            " Top K precision = 0.078021978021978, recall = 0.006777144059691541.\n",
            "Training on epoch 0 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.695692, and regularization loss is 0.002545.\n",
            " Top K precision = 0.09885057471264365, recall = 0.00810807699966025.\n",
            "\n",
            "Training on 0 epoch completed.\n",
            " Average bpr_loss on train set is 0.005569 for the current epoch.\n",
            " Training top K precision = 0.06899999999999995, recall = 0.006172750541756401.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.05899999999999992, recall = 0.006046670992002842.\n",
            "\n",
            "Training on the 1 epoch\n",
            "Training on epoch 1 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69573, and regularization loss is 0.002583.\n",
            " Top K precision = 0.08111111111111106, recall = 0.007705358059085852.\n",
            "Training on epoch 1 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.695241, and regularization loss is 0.002094.\n",
            " Top K precision = 0.08350515463917525, recall = 0.007596207151775263.\n",
            "Training on epoch 1 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.694185, and regularization loss is 0.001038.\n",
            " Top K precision = 0.09255319148936167, recall = 0.008462243852038517.\n",
            "Training on epoch 1 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.694432, and regularization loss is 0.001285.\n",
            " Top K precision = 0.08571428571428569, recall = 0.0076744517305775885.\n",
            "Training on epoch 1 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.694014, and regularization loss is 0.000867.\n",
            " Top K precision = 0.08556701030927834, recall = 0.006657131147431888.\n",
            "Training on epoch 1 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693906, and regularization loss is 0.000759.\n",
            " Top K precision = 0.08541666666666663, recall = 0.007977222281987056.\n",
            "Training on epoch 1 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.694018, and regularization loss is 0.00087.\n",
            " Top K precision = 0.08265306122448977, recall = 0.007062820062784286.\n",
            "Training on epoch 1 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693632, and regularization loss is 0.000485.\n",
            " Top K precision = 0.08749999999999997, recall = 0.006979701874440309.\n",
            "\n",
            "Training on 1 epoch completed.\n",
            " Average bpr_loss on train set is 0.00543 for the current epoch.\n",
            " Training top K precision = 0.08900000000000001, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 2 epoch\n",
            "Training on epoch 2 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693442, and regularization loss is 0.000295.\n",
            " Top K precision = 0.09368421052631576, recall = 0.006196246706956282.\n",
            "Training on epoch 2 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693936, and regularization loss is 0.000789.\n",
            " Top K precision = 0.10638297872340421, recall = 0.008628771745182671.\n",
            "Training on epoch 2 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693616, and regularization loss is 0.000469.\n",
            " Top K precision = 0.0927083333333333, recall = 0.00759738580139463.\n",
            "Training on epoch 2 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693604, and regularization loss is 0.000456.\n",
            " Top K precision = 0.08494623655913974, recall = 0.007491916291582737.\n",
            "Training on epoch 2 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693409, and regularization loss is 0.000261.\n",
            " Top K precision = 0.10459770114942528, recall = 0.008465772977719138.\n",
            "Training on epoch 2 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693375, and regularization loss is 0.000228.\n",
            " Top K precision = 0.0627659574468085, recall = 0.0061991518138035644.\n",
            "Training on epoch 2 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693677, and regularization loss is 0.000529.\n",
            " Top K precision = 0.0864583333333333, recall = 0.007748577002246549.\n",
            "Training on epoch 2 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693438, and regularization loss is 0.000291.\n",
            " Top K precision = 0.09896907216494844, recall = 0.007389845806506576.\n",
            "\n",
            "Training on 2 epoch completed.\n",
            " Average bpr_loss on train set is 0.005423 for the current epoch.\n",
            " Training top K precision = 0.08949999999999997, recall = 0.007591451866036865.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 3 epoch\n",
            "Training on epoch 3 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.08865979381443301, recall = 0.007574811880202433.\n",
            "Training on epoch 3 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693323, and regularization loss is 0.000176.\n",
            " Top K precision = 0.08910891089108909, recall = 0.006765170365570972.\n",
            "Training on epoch 3 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693252, and regularization loss is 0.000105.\n",
            " Top K precision = 0.06956521739130432, recall = 0.006772730851954841.\n",
            "Training on epoch 3 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693295, and regularization loss is 0.000148.\n",
            " Top K precision = 0.09062499999999994, recall = 0.008794565209926294.\n",
            "Training on epoch 3 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693339, and regularization loss is 0.000192.\n",
            " Top K precision = 0.08163265306122448, recall = 0.0069396967232443745.\n",
            "Training on epoch 3 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693341, and regularization loss is 0.000194.\n",
            " Top K precision = 0.10408163265306114, recall = 0.00898181210391049.\n",
            "Training on epoch 3 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693315, and regularization loss is 0.000168.\n",
            " Top K precision = 0.07731958762886595, recall = 0.008128619209395433.\n",
            "Training on epoch 3 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69325, and regularization loss is 0.000103.\n",
            " Top K precision = 0.09484536082474222, recall = 0.007037657852684652.\n",
            "\n",
            "Training on 3 epoch completed.\n",
            " Average bpr_loss on train set is 0.005422 for the current epoch.\n",
            " Training top K precision = 0.0885, recall = 0.007536249604952252.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 4 epoch\n",
            "Training on epoch 4 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693227, and regularization loss is 8e-05.\n",
            " Top K precision = 0.07640449438202249, recall = 0.008463944205329683.\n",
            "Training on epoch 4 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693187, and regularization loss is 4e-05.\n",
            " Top K precision = 0.08777777777777776, recall = 0.006589935088794662.\n",
            "Training on epoch 4 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693212, and regularization loss is 6.5e-05.\n",
            " Top K precision = 0.09117647058823525, recall = 0.00825913611155957.\n",
            "Training on epoch 4 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693196, and regularization loss is 4.9e-05.\n",
            " Top K precision = 0.09891304347826078, recall = 0.008182580570665916.\n",
            "Training on epoch 4 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693199, and regularization loss is 5.2e-05.\n",
            " Top K precision = 0.09468085106382976, recall = 0.006754981962609338.\n",
            "Training on epoch 4 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693213, and regularization loss is 6.6e-05.\n",
            " Top K precision = 0.08817204301075267, recall = 0.007752601422880906.\n",
            "Training on epoch 4 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.07956989247311824, recall = 0.00686846248766349.\n",
            "Training on epoch 4 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693192, and regularization loss is 4.5e-05.\n",
            " Top K precision = 0.07979797979797978, recall = 0.007923229395149339.\n",
            "\n",
            "Training on 4 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08849999999999994, recall = 0.007536450698903348.\n",
            "\n",
            "Training on the 5 epoch\n",
            "Training on epoch 5 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693182, and regularization loss is 3.5e-05.\n",
            " Top K precision = 0.09101123595505616, recall = 0.008746351244442433.\n",
            "Training on epoch 5 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.06914893617021274, recall = 0.007380590887051204.\n",
            "Training on epoch 5 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.0882978723404255, recall = 0.00797601265992671.\n",
            "Training on epoch 5 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.9e-05.\n",
            " Top K precision = 0.10760869565217387, recall = 0.008453133323332836.\n",
            "Training on epoch 5 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09387755102040812, recall = 0.006644970397676833.\n",
            "Training on epoch 5 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693264, and regularization loss is 0.000117.\n",
            " Top K precision = 0.08453608247422678, recall = 0.0070235540166571795.\n",
            "Training on epoch 5 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.08, recall = 0.007176337796985787.\n",
            "Training on epoch 5 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693152, and regularization loss is 4e-06.\n",
            " Top K precision = 0.08899999999999997, recall = 0.007871605497048796.\n",
            "\n",
            "Training on 5 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474773.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 6 epoch\n",
            "Training on epoch 6 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08989898989898985, recall = 0.00858026305487175.\n",
            "Training on epoch 6 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.10408163265306117, recall = 0.010072509911422264.\n",
            "Training on epoch 6 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09292929292929289, recall = 0.007580726498451839.\n",
            "Training on epoch 6 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693151, and regularization loss is 4e-06.\n",
            " Top K precision = 0.09278350515463914, recall = 0.008215883946126101.\n",
            "Training on epoch 6 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693252, and regularization loss is 0.000105.\n",
            " Top K precision = 0.09361702127659573, recall = 0.006901140204988293.\n",
            "Training on epoch 6 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.09687499999999998, recall = 0.009061787026798657.\n",
            "Training on epoch 6 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08279569892473118, recall = 0.00752740238943897.\n",
            "Training on epoch 6 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693215, and regularization loss is 6.8e-05.\n",
            " Top K precision = 0.07717391304347823, recall = 0.005490693291758064.\n",
            "\n",
            "Training on 6 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 7 epoch\n",
            "Training on epoch 7 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693187, and regularization loss is 4e-05.\n",
            " Top K precision = 0.09306930693069301, recall = 0.007453386059278577.\n",
            "Training on epoch 7 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
            " Top K precision = 0.09578947368421047, recall = 0.008093189854672823.\n",
            "Training on epoch 7 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.08367346938775506, recall = 0.008113625206903329.\n",
            "Training on epoch 7 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.1, recall = 0.008314858211805769.\n",
            "Training on epoch 7 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09081632653061221, recall = 0.007128173287108486.\n",
            "Training on epoch 7 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.07708333333333331, recall = 0.008305685836891601.\n",
            "Training on epoch 7 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69321, and regularization loss is 6.2e-05.\n",
            " Top K precision = 0.09702970297029696, recall = 0.007749558720270393.\n",
            "Training on epoch 7 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693186, and regularization loss is 3.8e-05.\n",
            " Top K precision = 0.09157894736842101, recall = 0.008885875961512104.\n",
            "\n",
            "Training on 7 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007545035863738516.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 8 epoch\n",
            "Training on epoch 8 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08409090909090906, recall = 0.007277163482221711.\n",
            "Training on epoch 8 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693186, and regularization loss is 3.9e-05.\n",
            " Top K precision = 0.07999999999999999, recall = 0.006042718952520241.\n",
            "Training on epoch 8 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.09999999999999994, recall = 0.006880131962367402.\n",
            "Training on epoch 8 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.08297872340425529, recall = 0.008650420825664648.\n",
            "Training on epoch 8 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08749999999999997, recall = 0.006799001663261281.\n",
            "Training on epoch 8 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693153, and regularization loss is 6e-06.\n",
            " Top K precision = 0.08673469387755098, recall = 0.007279224240918951.\n",
            "Training on epoch 8 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693202, and regularization loss is 5.5e-05.\n",
            " Top K precision = 0.08777777777777773, recall = 0.008182048537831357.\n",
            "Training on epoch 8 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.09894736842105258, recall = 0.008040956162965936.\n",
            "\n",
            "Training on 8 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007400506425159361.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.0075682778308968305.\n",
            "\n",
            "Training on the 9 epoch\n",
            "Training on epoch 9 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.0861702127659574, recall = 0.00784233055915166.\n",
            "Training on epoch 9 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.9e-05.\n",
            " Top K precision = 0.07765957446808507, recall = 0.0064915535608989485.\n",
            "Training on epoch 9 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.10638297872340424, recall = 0.007512667177846814.\n",
            "Training on epoch 9 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.07373737373737371, recall = 0.006682562821627517.\n",
            "Training on epoch 9 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.09690721649484531, recall = 0.008056929990114661.\n",
            "Training on epoch 9 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.10113636363636362, recall = 0.009398917521482891.\n",
            "Training on epoch 9 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693178, and regularization loss is 3.1e-05.\n",
            " Top K precision = 0.10434782608695647, recall = 0.008693071576271787.\n",
            "Training on epoch 9 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08019801980198016, recall = 0.00636280402529352.\n",
            "\n",
            "Training on 9 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474772.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 10 epoch\n",
            "Training on epoch 10 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.08315789473684203, recall = 0.006596516333707446.\n",
            "Training on epoch 10 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.10215053763440853, recall = 0.008717926649927656.\n",
            "Training on epoch 10 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 8e-06.\n",
            " Top K precision = 0.07920792079207917, recall = 0.00728841800264841.\n",
            "Training on epoch 10 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.1061224489795918, recall = 0.008999177734412.\n",
            "Training on epoch 10 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.1073684210526315, recall = 0.007664598584736204.\n",
            "Training on epoch 10 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.09215686274509804, recall = 0.006539284420095337.\n",
            "Training on epoch 10 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09473684210526312, recall = 0.008127134840816262.\n",
            "Training on epoch 10 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08021978021978021, recall = 0.007054268834538662.\n",
            "\n",
            "Training on 10 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999994, recall = 0.0075120431555121625.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 11 epoch\n",
            "Training on epoch 11 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08877551020408159, recall = 0.007639742212465396.\n",
            "Training on epoch 11 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08804347826086954, recall = 0.0075987431291463054.\n",
            "Training on epoch 11 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08651685393258425, recall = 0.007521087026926617.\n",
            "Training on epoch 11 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09673913043478258, recall = 0.008385383853625264.\n",
            "Training on epoch 11 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09673913043478262, recall = 0.006777618766884513.\n",
            "Training on epoch 11 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0842696629213483, recall = 0.006302853013967166.\n",
            "Training on epoch 11 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08854166666666664, recall = 0.00866910607739296.\n",
            "Training on epoch 11 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08021978021978021, recall = 0.006288685751713257.\n",
            "\n",
            "Training on 11 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08949999999999995, recall = 0.007743957312659962.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.0075682778308968305.\n",
            "\n",
            "Training on the 12 epoch\n",
            "Training on epoch 12 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09387755102040812, recall = 0.007297878324997023.\n",
            "Training on epoch 12 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08229166666666664, recall = 0.007407003103677474.\n",
            "Training on epoch 12 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08125, recall = 0.008260694082135506.\n",
            "Training on epoch 12 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.07666666666666667, recall = 0.0071317229215640925.\n",
            "Training on epoch 12 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10294117647058816, recall = 0.007979793811215937.\n",
            "Training on epoch 12 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09687499999999995, recall = 0.007403386936436252.\n",
            "Training on epoch 12 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08510638297872339, recall = 0.007996247314410744.\n",
            "Training on epoch 12 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0916666666666666, recall = 0.008509550159158498.\n",
            "\n",
            "Training on 12 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999988, recall = 0.00754993820521329.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999997, recall = 0.007453788178601698.\n",
            "\n",
            "Training on the 13 epoch\n",
            "Training on epoch 13 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08686868686868682, recall = 0.006555707988056193.\n",
            "Training on epoch 13 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09340659340659337, recall = 0.008973070221448576.\n",
            "Training on epoch 13 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08191489361702121, recall = 0.006632661936219802.\n",
            "Training on epoch 13 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0833333333333333, recall = 0.007255120390084198.\n",
            "Training on epoch 13 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08556701030927834, recall = 0.006385008129891932.\n",
            "Training on epoch 13 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08736842105263153, recall = 0.008444382427274844.\n",
            "Training on epoch 13 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08989898989898988, recall = 0.007556345015432528.\n",
            "Training on epoch 13 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.0824742268041237, recall = 0.008444109000341633.\n",
            "\n",
            "Training on 13 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08699999999999992, recall = 0.007402094705630701.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08549999999999994, recall = 0.007382303572585126.\n",
            "\n",
            "Training on the 14 epoch\n",
            "Training on epoch 14 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08210526315789472, recall = 0.00662829885647042.\n",
            "Training on epoch 14 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09789473684210523, recall = 0.007563955951182802.\n",
            "Training on epoch 14 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07499999999999996, recall = 0.008101049904029576.\n",
            "Training on epoch 14 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10105263157894735, recall = 0.008478389548916618.\n",
            "Training on epoch 14 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0911111111111111, recall = 0.009286996071579635.\n",
            "Training on epoch 14 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08989898989898985, recall = 0.008428326477520369.\n",
            "Training on epoch 14 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09354838709677414, recall = 0.00795167452635935.\n",
            "Training on epoch 14 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.07722772277227723, recall = 0.007484444546333014.\n",
            "\n",
            "Training on 14 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999995, recall = 0.00745780775478212.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.007567896215066018.\n",
            "\n",
            "Training on the 15 epoch\n",
            "Training on epoch 15 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.10312499999999995, recall = 0.00951247785279464.\n",
            "Training on epoch 15 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08191489361702124, recall = 0.007904565623499376.\n",
            "Training on epoch 15 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09684210526315787, recall = 0.008234131675469974.\n",
            "Training on epoch 15 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08823529411764702, recall = 0.008087216159664467.\n",
            "Training on epoch 15 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08191489361702126, recall = 0.008264121390867938.\n",
            "Training on epoch 15 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09062499999999996, recall = 0.005298683011760096.\n",
            "Training on epoch 15 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.10219780219780215, recall = 0.008408738421510596.\n",
            "Training on epoch 15 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.08888888888888885, recall = 0.009196799835147639.\n",
            "\n",
            "Training on 15 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08399999999999995, recall = 0.007086022102093129.\n",
            "\n",
            "Training on the 16 epoch\n",
            "Training on epoch 16 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08061224489795916, recall = 0.007884513854122286.\n",
            "Training on epoch 16 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.10404040404040402, recall = 0.007470903396045921.\n",
            "Training on epoch 16 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09555555555555552, recall = 0.007267425793282106.\n",
            "Training on epoch 16 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.0918367346938775, recall = 0.006112804936558255.\n",
            "Training on epoch 16 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08631578947368418, recall = 0.007265023371293441.\n",
            "Training on epoch 16 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07765957446808508, recall = 0.006043826186041497.\n",
            "Training on epoch 16 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07789473684210525, recall = 0.007073017283193072.\n",
            "Training on epoch 16 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09270833333333328, recall = 0.00704979985777919.\n",
            "\n",
            "Training on 16 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999993, recall = 0.007536249604952255.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007225836370732955.\n",
            "\n",
            "Training on the 17 epoch\n",
            "Training on epoch 17 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.07525773195876286, recall = 0.0073953086841730235.\n",
            "Training on epoch 17 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.07422680412371134, recall = 0.00642898039083968.\n",
            "Training on epoch 17 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08910891089108909, recall = 0.007650770785287251.\n",
            "Training on epoch 17 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09456521739130432, recall = 0.008300843484742165.\n",
            "Training on epoch 17 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.09677419354838708, recall = 0.009004571838054268.\n",
            "Training on epoch 17 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08571428571428566, recall = 0.0074221912079728255.\n",
            "Training on epoch 17 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.07727272727272726, recall = 0.006468011818968345.\n",
            "Training on epoch 17 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.06236559139784945, recall = 0.006969215838520844.\n",
            "\n",
            "Training on 17 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007520310589013236.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.0074818490505517.\n",
            "\n",
            "Training on the 18 epoch\n",
            "Training on epoch 18 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09387755102040814, recall = 0.008228815100176678.\n",
            "Training on epoch 18 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08478260869565213, recall = 0.007668460750152955.\n",
            "Training on epoch 18 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.0902912621359223, recall = 0.007912557732041307.\n",
            "Training on epoch 18 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08842105263157891, recall = 0.0068064940631606605.\n",
            "Training on epoch 18 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09139784946236554, recall = 0.00885640072684625.\n",
            "Training on epoch 18 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08510638297872337, recall = 0.008450158579751054.\n",
            "Training on epoch 18 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0864583333333333, recall = 0.008115248826912879.\n",
            "Training on epoch 18 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09255319148936167, recall = 0.008893727485165598.\n",
            "\n",
            "Training on 18 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08649999999999991, recall = 0.007457902744127977.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08449999999999994, recall = 0.007369077031281982.\n",
            "\n",
            "Training on the 19 epoch\n",
            "Training on epoch 19 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09777777777777778, recall = 0.008164891223736747.\n",
            "Training on epoch 19 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09659090909090905, recall = 0.007753345194110723.\n",
            "Training on epoch 19 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07692307692307689, recall = 0.006417214753347937.\n",
            "Training on epoch 19 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09299999999999996, recall = 0.0079626645785468.\n",
            "Training on epoch 19 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09888888888888883, recall = 0.008283971053504771.\n",
            "Training on epoch 19 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09021739130434778, recall = 0.00788414764701628.\n",
            "Training on epoch 19 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.09393939393939385, recall = 0.007993454471165224.\n",
            "Training on epoch 19 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09038461538461538, recall = 0.008170483801724663.\n",
            "\n",
            "Training on 19 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474779.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.007545574786494588.\n",
            "\n",
            "Training on the 20 epoch\n",
            "Training on epoch 20 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.0929292929292929, recall = 0.007393291290145266.\n",
            "Training on epoch 20 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08736842105263153, recall = 0.007327260122669884.\n",
            "Training on epoch 20 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09333333333333332, recall = 0.007860369205501361.\n",
            "Training on epoch 20 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09166666666666662, recall = 0.00834323769963929.\n",
            "Training on epoch 20 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09368421052631576, recall = 0.007965687005805744.\n",
            "Training on epoch 20 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08043478260869565, recall = 0.007204378263933473.\n",
            "Training on epoch 20 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08842105263157893, recall = 0.006249861181968009.\n",
            "Training on epoch 20 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.08172043010752687, recall = 0.00678910625166328.\n",
            "\n",
            "Training on 20 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08599999999999997, recall = 0.0073559871632005655.\n",
            "\n",
            "Training on the 21 epoch\n",
            "Training on epoch 21 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.06593406593406592, recall = 0.007847587509955753.\n",
            "Training on epoch 21 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.10309278350515458, recall = 0.008171139411066147.\n",
            "Training on epoch 21 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.1051546391752577, recall = 0.007813835312844226.\n",
            "Training on epoch 21 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08762886597938142, recall = 0.007709462812426965.\n",
            "Training on epoch 21 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08888888888888886, recall = 0.006829334851910913.\n",
            "Training on epoch 21 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0838095238095238, recall = 0.007440077150520664.\n",
            "Training on epoch 21 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.0816091954022988, recall = 0.007270294658558979.\n",
            "Training on epoch 21 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09574468085106382, recall = 0.00739536476468426.\n",
            "\n",
            "Training on 21 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999998, recall = 0.007570097319661069.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08849999999999994, recall = 0.007536450698903348.\n",
            "\n",
            "Training on the 22 epoch\n",
            "Training on epoch 22 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.10224719101123593, recall = 0.00808103333203085.\n",
            "Training on epoch 22 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08921568627450978, recall = 0.007127069751947584.\n",
            "Training on epoch 22 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08229166666666667, recall = 0.007435763002110716.\n",
            "Training on epoch 22 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08571428571428569, recall = 0.008118463607723014.\n",
            "Training on epoch 22 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09999999999999995, recall = 0.007127192355080373.\n",
            "Training on epoch 22 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.095, recall = 0.00738301222037222.\n",
            "Training on epoch 22 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09777777777777773, recall = 0.00805969218031341.\n",
            "Training on epoch 22 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09239130434782607, recall = 0.007711182188060289.\n",
            "\n",
            "Training on 22 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007536249604952255.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 23 epoch\n",
            "Training on epoch 23 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09666666666666664, recall = 0.009172143431258144.\n",
            "Training on epoch 23 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07628865979381441, recall = 0.007179215277291652.\n",
            "Training on epoch 23 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09393939393939388, recall = 0.008564674884160664.\n",
            "Training on epoch 23 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.09468085106382977, recall = 0.007240721392948056.\n",
            "Training on epoch 23 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.1030927835051546, recall = 0.008080488763533728.\n",
            "Training on epoch 23 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08571428571428569, recall = 0.006499350493141507.\n",
            "Training on epoch 23 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.09900990099009899, recall = 0.008748967080115985.\n",
            "Training on epoch 23 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.10319148936170207, recall = 0.00708623688177708.\n",
            "\n",
            "Training on 23 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08599999999999997, recall = 0.007273812990680711.\n",
            "\n",
            "Training on the 24 epoch\n",
            "Training on epoch 24 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09255319148936167, recall = 0.00818512871903721.\n",
            "Training on epoch 24 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.07888888888888887, recall = 0.007022495439008918.\n",
            "Training on epoch 24 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09072164948453604, recall = 0.008064578742624536.\n",
            "Training on epoch 24 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08999999999999993, recall = 0.0070636018445304075.\n",
            "Training on epoch 24 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08247422680412367, recall = 0.007878952529886999.\n",
            "Training on epoch 24 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08602150537634405, recall = 0.008010982167574843.\n",
            "Training on epoch 24 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09230769230769231, recall = 0.0087644319713807.\n",
            "Training on epoch 24 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.07157894736842102, recall = 0.006228012888929813.\n",
            "\n",
            "Training on 24 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999995, recall = 0.007536520160797157.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007403646912340229.\n",
            "\n",
            "Training on the 25 epoch\n",
            "Training on epoch 25 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0793478260869565, recall = 0.006495454145531689.\n",
            "Training on epoch 25 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08279569892473118, recall = 0.008904506304571496.\n",
            "Training on epoch 25 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09583333333333331, recall = 0.008853938156883885.\n",
            "Training on epoch 25 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08390804597701149, recall = 0.006479790849701303.\n",
            "Training on epoch 25 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07526881720430102, recall = 0.004894268383943141.\n",
            "Training on epoch 25 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.09302325581395345, recall = 0.008343584737576064.\n",
            "Training on epoch 25 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0838383838383838, recall = 0.0064096322686276975.\n",
            "Training on epoch 25 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09595959595959595, recall = 0.008048272495931236.\n",
            "\n",
            "Training on 25 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007528832007714297.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007499767212330569.\n",
            "\n",
            "Training on the 26 epoch\n",
            "Training on epoch 26 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09591836734693872, recall = 0.008104084886055013.\n",
            "Training on epoch 26 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08315789473684208, recall = 0.008584499224877955.\n",
            "Training on epoch 26 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08085106382978717, recall = 0.005610798265573766.\n",
            "Training on epoch 26 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0944444444444444, recall = 0.007373472847854176.\n",
            "Training on epoch 26 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08586956521739128, recall = 0.008706091325443083.\n",
            "Training on epoch 26 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09374999999999996, recall = 0.008071135632115844.\n",
            "Training on epoch 26 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08058252427184465, recall = 0.006473852952667784.\n",
            "Training on epoch 26 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09687499999999993, recall = 0.00700561510050372.\n",
            "\n",
            "Training on 26 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999993, recall = 0.007518121720970708.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08549999999999995, recall = 0.007403632068675579.\n",
            "\n",
            "Training on the 27 epoch\n",
            "Training on epoch 27 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.07812499999999999, recall = 0.0073701130555017475.\n",
            "Training on epoch 27 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.07319587628865978, recall = 0.005435929835786411.\n",
            "Training on epoch 27 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08723404255319145, recall = 0.006083558630840287.\n",
            "Training on epoch 27 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.07999999999999997, recall = 0.008100227784068763.\n",
            "Training on epoch 27 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0936170212765957, recall = 0.008640906124990563.\n",
            "Training on epoch 27 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09797979797979792, recall = 0.008495913628894384.\n",
            "Training on epoch 27 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08095238095238093, recall = 0.006947250533522016.\n",
            "Training on epoch 27 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09139784946236557, recall = 0.00859586938426163.\n",
            "\n",
            "Training on 27 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999991, recall = 0.007496434328272208.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08499999999999992, recall = 0.0074097274377860475.\n",
            "\n",
            "Training on the 28 epoch\n",
            "Training on epoch 28 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.07872340425531911, recall = 0.0063886563347765515.\n",
            "Training on epoch 28 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08924731182795696, recall = 0.00799731208178666.\n",
            "Training on epoch 28 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.10645161290322577, recall = 0.009340469844297764.\n",
            "Training on epoch 28 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.09255319148936167, recall = 0.007972692845257718.\n",
            "Training on epoch 28 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07799999999999996, recall = 0.007486874683282522.\n",
            "Training on epoch 28 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09791666666666658, recall = 0.007363790076209975.\n",
            "Training on epoch 28 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08809523809523812, recall = 0.007267130979798493.\n",
            "Training on epoch 28 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09247311827956989, recall = 0.007671717839640148.\n",
            "\n",
            "Training on 28 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007541706626631087.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 29 epoch\n",
            "Training on epoch 29 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08736842105263155, recall = 0.00709811117726828.\n",
            "Training on epoch 29 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.10102040816326524, recall = 0.006508382796602855.\n",
            "Training on epoch 29 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.06999999999999998, recall = 0.0067984344867977435.\n",
            "Training on epoch 29 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0868131868131868, recall = 0.0077733916490585.\n",
            "Training on epoch 29 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.07599999999999997, recall = 0.00743617698351533.\n",
            "Training on epoch 29 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0739583333333333, recall = 0.0078111196469659.\n",
            "Training on epoch 29 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.10202020202020198, recall = 0.007358586254271257.\n",
            "Training on epoch 29 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08152173913043476, recall = 0.007872189215321197.\n",
            "\n",
            "Training on 29 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007558772127474773.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08599999999999997, recall = 0.007444282475179645.\n",
            "\n",
            "Training on the 30 epoch\n",
            "Training on epoch 30 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08571428571428569, recall = 0.007179698614935925.\n",
            "Training on epoch 30 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09784946236559136, recall = 0.008063061734070115.\n",
            "Training on epoch 30 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08936170212765958, recall = 0.007322767367233183.\n",
            "Training on epoch 30 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.10217391304347824, recall = 0.006792973408755302.\n",
            "Training on epoch 30 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.07252747252747248, recall = 0.006871400658754321.\n",
            "Training on epoch 30 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09368421052631573, recall = 0.008005242069627987.\n",
            "Training on epoch 30 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09157894736842102, recall = 0.009959602767329526.\n",
            "Training on epoch 30 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09803921568627448, recall = 0.008577649048662093.\n",
            "\n",
            "Training on 30 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.0075203105890132346.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999997, recall = 0.007444297318844293.\n",
            "\n",
            "Training on the 31 epoch\n",
            "Training on epoch 31 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09509803921568621, recall = 0.0066442442647707605.\n",
            "Training on epoch 31 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.09574468085106379, recall = 0.008342911798868823.\n",
            "Training on epoch 31 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0865979381443299, recall = 0.0063939294270108455.\n",
            "Training on epoch 31 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08315789473684208, recall = 0.008252327330782452.\n",
            "Training on epoch 31 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09591836734693872, recall = 0.007665210190729315.\n",
            "Training on epoch 31 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08989898989898985, recall = 0.006471533382859742.\n",
            "Training on epoch 31 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09999999999999992, recall = 0.007990289501316767.\n",
            "Training on epoch 31 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09090909090909091, recall = 0.006641874871888126.\n",
            "\n",
            "Training on 31 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999991, recall = 0.007558772127474777.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 32 epoch\n",
            "Training on epoch 32 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.10505050505050498, recall = 0.009034514010114496.\n",
            "Training on epoch 32 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08999999999999997, recall = 0.00786042961492149.\n",
            "Training on epoch 32 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09387755102040812, recall = 0.0077370867023176925.\n",
            "Training on epoch 32 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08842105263157891, recall = 0.00681544623061481.\n",
            "Training on epoch 32 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.08947368421052626, recall = 0.007427896979037953.\n",
            "Training on epoch 32 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08817204301075267, recall = 0.008030894733411849.\n",
            "Training on epoch 32 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 7e-06.\n",
            " Top K precision = 0.07083333333333335, recall = 0.006901511224692176.\n",
            "Training on epoch 32 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08617021276595742, recall = 0.006369374107667394.\n",
            "\n",
            "Training on 32 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08249999999999995, recall = 0.007217953571909571.\n",
            "\n",
            "Training on the 33 epoch\n",
            "Training on epoch 33 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.10899999999999997, recall = 0.008469926945310768.\n",
            "Training on epoch 33 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.07872340425531911, recall = 0.007031889716315847.\n",
            "Training on epoch 33 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.11395348837209297, recall = 0.008169062905510257.\n",
            "Training on epoch 33 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09456521739130434, recall = 0.006632032973990667.\n",
            "Training on epoch 33 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.10227272727272724, recall = 0.007408667456044787.\n",
            "Training on epoch 33 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.1044943820224719, recall = 0.008451800487586907.\n",
            "Training on epoch 33 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.10101010101010095, recall = 0.0080926308331557.\n",
            "Training on epoch 33 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.08709677419354835, recall = 0.007962205154945575.\n",
            "\n",
            "Training on 33 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999994, recall = 0.007539975134993574.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999997, recall = 0.007391421973464922.\n",
            "\n",
            "Training on the 34 epoch\n",
            "Training on epoch 34 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0865979381443299, recall = 0.00585287260695515.\n",
            "Training on epoch 34 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09699999999999996, recall = 0.00872175039799079.\n",
            "Training on epoch 34 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09368421052631576, recall = 0.007878725546746868.\n",
            "Training on epoch 34 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08571428571428569, recall = 0.006199392898290163.\n",
            "Training on epoch 34 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08989898989898987, recall = 0.006418777185527681.\n",
            "Training on epoch 34 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08901098901098897, recall = 0.007929992819164683.\n",
            "Training on epoch 34 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08666666666666664, recall = 0.008840254762073953.\n",
            "Training on epoch 34 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08282828282828282, recall = 0.007273992587589831.\n",
            "\n",
            "Training on 34 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999998, recall = 0.007521468779287146.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08599999999999994, recall = 0.007475317334579957.\n",
            "\n",
            "Training on the 35 epoch\n",
            "Training on epoch 35 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09473684210526312, recall = 0.00794525497018497.\n",
            "Training on epoch 35 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09021739130434782, recall = 0.008099558523358728.\n",
            "Training on epoch 35 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08437499999999998, recall = 0.006400586291341462.\n",
            "Training on epoch 35 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09444444444444443, recall = 0.008471642757086985.\n",
            "Training on epoch 35 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.08989898989898987, recall = 0.00655524640972526.\n",
            "Training on epoch 35 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09294117647058824, recall = 0.007556530706234343.\n",
            "Training on epoch 35 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.07978723404255317, recall = 0.0070336930205076005.\n",
            "Training on epoch 35 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09431818181818179, recall = 0.00802639415147343.\n",
            "\n",
            "Training on 35 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999995, recall = 0.007481849050551703.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007475665106410309.\n",
            "\n",
            "Training on the 36 epoch\n",
            "Training on epoch 36 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.08118811881188116, recall = 0.007120120050401181.\n",
            "Training on epoch 36 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08333333333333331, recall = 0.007767679459987189.\n",
            "Training on epoch 36 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09595959595959594, recall = 0.008455913067398917.\n",
            "Training on epoch 36 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09278350515463914, recall = 0.007876840242464319.\n",
            "Training on epoch 36 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.0958762886597938, recall = 0.006799287397195325.\n",
            "Training on epoch 36 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07999999999999996, recall = 0.007169878737682008.\n",
            "Training on epoch 36 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08659793814432984, recall = 0.00836071988771025.\n",
            "Training on epoch 36 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09072164948453601, recall = 0.008055899372187156.\n",
            "\n",
            "Training on 36 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08900000000000001, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.0075682778308968305.\n",
            "\n",
            "Training on the 37 epoch\n",
            "Training on epoch 37 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0936170212765957, recall = 0.00848122716741414.\n",
            "Training on epoch 37 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.09999999999999995, recall = 0.006782060948246699.\n",
            "Training on epoch 37 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08799999999999994, recall = 0.007667466850030561.\n",
            "Training on epoch 37 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09354838709677417, recall = 0.007321218197155648.\n",
            "Training on epoch 37 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08762886597938141, recall = 0.007450012340992038.\n",
            "Training on epoch 37 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08444444444444443, recall = 0.00737477200733278.\n",
            "Training on epoch 37 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.07979797979797977, recall = 0.0073953235900113354.\n",
            "Training on epoch 37 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08829787234042552, recall = 0.007588611125905817.\n",
            "\n",
            "Training on 37 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999993, recall = 0.007486765276186703.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.00761246134361222.\n",
            "\n",
            "Training on the 38 epoch\n",
            "Training on epoch 38 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09361702127659571, recall = 0.0073196702549098.\n",
            "Training on epoch 38 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09999999999999996, recall = 0.007033286310119726.\n",
            "Training on epoch 38 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09411764705882347, recall = 0.007152996526089379.\n",
            "Training on epoch 38 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08470588235294116, recall = 0.006669772462553036.\n",
            "Training on epoch 38 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.075, recall = 0.007617623312359708.\n",
            "Training on epoch 38 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08299999999999998, recall = 0.006774023317129877.\n",
            "Training on epoch 38 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08709677419354835, recall = 0.00745831177134199.\n",
            "Training on epoch 38 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08924731182795696, recall = 0.006654625981087835.\n",
            "\n",
            "Training on 38 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999997, recall = 0.007441620620300892.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.007481467434720887.\n",
            "\n",
            "Training on the 39 epoch\n",
            "Training on epoch 39 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.07938144329896904, recall = 0.007171259701495108.\n",
            "Training on epoch 39 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07959183673469386, recall = 0.006734305673770498.\n",
            "Training on epoch 39 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.1030927835051546, recall = 0.009429003308124375.\n",
            "Training on epoch 39 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 7e-06.\n",
            " Top K precision = 0.09999999999999995, recall = 0.00768140429683007.\n",
            "Training on epoch 39 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08817204301075264, recall = 0.007445733129403046.\n",
            "Training on epoch 39 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09782608695652169, recall = 0.007834013054563221.\n",
            "Training on epoch 39 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09569892473118279, recall = 0.00786564856265585.\n",
            "Training on epoch 39 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.10108695652173912, recall = 0.007891810453708556.\n",
            "\n",
            "Training on 39 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007536450698903347.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999997, recall = 0.007444297318844293.\n",
            "\n",
            "Training on the 40 epoch\n",
            "Training on epoch 40 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08947368421052627, recall = 0.006958094102809461.\n",
            "Training on epoch 40 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.07608695652173914, recall = 0.007269509432608413.\n",
            "Training on epoch 40 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08125, recall = 0.007261718756346701.\n",
            "Training on epoch 40 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.07604166666666666, recall = 0.006570587827558039.\n",
            "Training on epoch 40 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.07419354838709676, recall = 0.007804173958085415.\n",
            "Training on epoch 40 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.07010309278350511, recall = 0.006452283023317529.\n",
            "Training on epoch 40 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0922222222222222, recall = 0.007236710280640816.\n",
            "Training on epoch 40 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.07916666666666665, recall = 0.006290261111515555.\n",
            "\n",
            "Training on 40 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007549373631234174.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08849999999999997, recall = 0.007518121720970712.\n",
            "\n",
            "Training on the 41 epoch\n",
            "Training on epoch 41 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08602150537634407, recall = 0.007151739652482484.\n",
            "Training on epoch 41 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0826086956521739, recall = 0.006792161991990943.\n",
            "Training on epoch 41 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0938144329896907, recall = 0.006289671420464955.\n",
            "Training on epoch 41 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0694736842105263, recall = 0.00516245235836732.\n",
            "Training on epoch 41 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693177, and regularization loss is 3e-05.\n",
            " Top K precision = 0.08749999999999997, recall = 0.008561609541972237.\n",
            "Training on epoch 41 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09780219780219777, recall = 0.009090370837415433.\n",
            "Training on epoch 41 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09690721649484531, recall = 0.009542514635464188.\n",
            "Training on epoch 41 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.07525773195876287, recall = 0.007234069586564777.\n",
            "\n",
            "Training on 41 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08949999999999995, recall = 0.00757365307985573.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 42 epoch\n",
            "Training on epoch 42 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08585858585858583, recall = 0.00840687920973383.\n",
            "Training on epoch 42 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08163265306122448, recall = 0.006550921930407695.\n",
            "Training on epoch 42 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.09479166666666666, recall = 0.008121560597888168.\n",
            "Training on epoch 42 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.0891304347826087, recall = 0.0058234269569016886.\n",
            "Training on epoch 42 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.08585858585858583, recall = 0.007612679684695566.\n",
            "Training on epoch 42 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.10645161290322577, recall = 0.008586133844894395.\n",
            "Training on epoch 42 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08958333333333333, recall = 0.00720252038259517.\n",
            "Training on epoch 42 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08901098901098899, recall = 0.007982529724135592.\n",
            "\n",
            "Training on 42 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08299999999999993, recall = 0.0072161872977662325.\n",
            "\n",
            "Training on the 43 epoch\n",
            "Training on epoch 43 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08762886597938138, recall = 0.0077104996491712385.\n",
            "Training on epoch 43 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.10631578947368418, recall = 0.007687111246249303.\n",
            "Training on epoch 43 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693177, and regularization loss is 3e-05.\n",
            " Top K precision = 0.07977528089887637, recall = 0.0060716075440202955.\n",
            "Training on epoch 43 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07204301075268814, recall = 0.006287923977862658.\n",
            "Training on epoch 43 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.1010869565217391, recall = 0.007928213318338517.\n",
            "Training on epoch 43 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09555555555555553, recall = 0.00937201569937616.\n",
            "Training on epoch 43 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08977272727272723, recall = 0.007761757883202976.\n",
            "Training on epoch 43 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10937499999999996, recall = 0.009852282819983843.\n",
            "\n",
            "Training on 43 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007488349592263508.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999997, recall = 0.007473536657231126.\n",
            "\n",
            "Training on the 44 epoch\n",
            "Training on epoch 44 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.075531914893617, recall = 0.007597905274599026.\n",
            "Training on epoch 44 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09090909090909087, recall = 0.006914634379076453.\n",
            "Training on epoch 44 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08131868131868128, recall = 0.007028981687166564.\n",
            "Training on epoch 44 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.07419354838709676, recall = 0.008304867255828392.\n",
            "Training on epoch 44 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08415841584158416, recall = 0.006860931174472296.\n",
            "Training on epoch 44 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09574468085106377, recall = 0.007654839142692047.\n",
            "Training on epoch 44 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07878787878787877, recall = 0.006638115265012798.\n",
            "Training on epoch 44 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0833333333333333, recall = 0.007090011357103513.\n",
            "\n",
            "Training on 44 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08299999999999995, recall = 0.007207798127444024.\n",
            "\n",
            "Training on the 45 epoch\n",
            "Training on epoch 45 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0797979797979798, recall = 0.007478692824728565.\n",
            "Training on epoch 45 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.07227722772277227, recall = 0.007034742556240001.\n",
            "Training on epoch 45 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09230769230769227, recall = 0.006235721309622352.\n",
            "Training on epoch 45 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08105263157894735, recall = 0.007619162469830348.\n",
            "Training on epoch 45 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09687499999999998, recall = 0.007662206871915927.\n",
            "Training on epoch 45 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08631578947368418, recall = 0.0074010305653114455.\n",
            "Training on epoch 45 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.0857142857142857, recall = 0.0064728337325062335.\n",
            "Training on epoch 45 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09247311827956985, recall = 0.007943790200034405.\n",
            "\n",
            "Training on 45 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999991, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999997, recall = 0.00745650741405495.\n",
            "\n",
            "Training on the 46 epoch\n",
            "Training on epoch 46 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09574468085106376, recall = 0.008266831753772907.\n",
            "Training on epoch 46 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08269230769230768, recall = 0.007376823656340738.\n",
            "Training on epoch 46 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09999999999999996, recall = 0.007873424191975932.\n",
            "Training on epoch 46 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0967741935483871, recall = 0.007474231765052742.\n",
            "Training on epoch 46 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0840425531914893, recall = 0.007881939729714408.\n",
            "Training on epoch 46 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08877551020408159, recall = 0.007815588063054543.\n",
            "Training on epoch 46 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09062499999999996, recall = 0.006808888853126696.\n",
            "Training on epoch 46 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.08854166666666662, recall = 0.008000001428946161.\n",
            "\n",
            "Training on 46 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007587322276432076.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007432072379968988.\n",
            "\n",
            "Training on the 47 epoch\n",
            "Training on epoch 47 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09793814432989686, recall = 0.008835493605037489.\n",
            "Training on epoch 47 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.11630434782608691, recall = 0.008118842024041928.\n",
            "Training on epoch 47 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.07666666666666665, recall = 0.0061104892350077565.\n",
            "Training on epoch 47 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0947368421052631, recall = 0.00832228416489359.\n",
            "Training on epoch 47 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09239130434782603, recall = 0.008739784791352102.\n",
            "Training on epoch 47 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08369565217391305, recall = 0.006526840494427655.\n",
            "Training on epoch 47 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08222222222222221, recall = 0.005754894959376001.\n",
            "Training on epoch 47 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.07684210526315788, recall = 0.007534378215629722.\n",
            "\n",
            "Training on 47 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999991, recall = 0.007695031814935071.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007496666361046503.\n",
            "\n",
            "Training on the 48 epoch\n",
            "Training on epoch 48 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08777777777777776, recall = 0.008488192514778479.\n",
            "Training on epoch 48 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08415841584158416, recall = 0.007798977799696657.\n",
            "Training on epoch 48 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09230769230769227, recall = 0.007311541038568388.\n",
            "Training on epoch 48 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09677419354838708, recall = 0.0074654083359392935.\n",
            "Training on epoch 48 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.07187500000000001, recall = 0.006212743199923759.\n",
            "Training on epoch 48 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08118811881188116, recall = 0.007228149624877034.\n",
            "Training on epoch 48 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08723404255319148, recall = 0.006381013286061857.\n",
            "Training on epoch 48 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08282828282828278, recall = 0.006984926494977804.\n",
            "\n",
            "Training on 48 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.0885, recall = 0.007538025239507969.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.0074818490505517.\n",
            "\n",
            "Training on the 49 epoch\n",
            "Training on epoch 49 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 8e-06.\n",
            " Top K precision = 0.08599999999999998, recall = 0.007267828721808547.\n",
            "Training on epoch 49 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09999999999999996, recall = 0.008109828517600202.\n",
            "Training on epoch 49 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08735632183908044, recall = 0.007042976188412253.\n",
            "Training on epoch 49 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07142857142857144, recall = 0.007320768308945331.\n",
            "Training on epoch 49 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09519230769230765, recall = 0.008024833323841997.\n",
            "Training on epoch 49 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0828282828282828, recall = 0.0059023403292857565.\n",
            "Training on epoch 49 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08421052631578947, recall = 0.006699894424346134.\n",
            "Training on epoch 49 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09999999999999996, recall = 0.007214415555564818.\n",
            "\n",
            "Training on 49 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474779.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999997, recall = 0.007367374241921216.\n",
            "\n",
            "Training on the 50 epoch\n",
            "Training on epoch 50 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.095, recall = 0.00855197164611253.\n",
            "Training on epoch 50 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08617021276595739, recall = 0.006280847660000918.\n",
            "Training on epoch 50 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09893617021276593, recall = 0.00768151021690961.\n",
            "Training on epoch 50 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09787234042553193, recall = 0.006383135303426872.\n",
            "Training on epoch 50 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08399999999999996, recall = 0.007386846374672519.\n",
            "Training on epoch 50 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09199999999999996, recall = 0.006640666209805996.\n",
            "Training on epoch 50 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.0927083333333333, recall = 0.007026420572843413.\n",
            "Training on epoch 50 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08039215686274506, recall = 0.007647374688579181.\n",
            "\n",
            "Training on 50 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474775.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999995, recall = 0.007459116805826504.\n",
            "\n",
            "Training on the 51 epoch\n",
            "Training on epoch 51 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08351648351648353, recall = 0.007048012674260803.\n",
            "Training on epoch 51 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.0833333333333333, recall = 0.008475370644488648.\n",
            "Training on epoch 51 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.10510204081632649, recall = 0.00882840665549839.\n",
            "Training on epoch 51 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09574468085106379, recall = 0.008690390284530867.\n",
            "Training on epoch 51 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08645833333333332, recall = 0.0067996039704887766.\n",
            "Training on epoch 51 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.07888888888888887, recall = 0.007606252603790119.\n",
            "Training on epoch 51 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09299999999999997, recall = 0.007169367418852862.\n",
            "Training on epoch 51 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09574468085106379, recall = 0.007233847647472564.\n",
            "\n",
            "Training on 51 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999998, recall = 0.007547710180572121.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999995, recall = 0.007594106834972077.\n",
            "\n",
            "Training on the 52 epoch\n",
            "Training on epoch 52 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08924731182795696, recall = 0.007289957150361921.\n",
            "Training on epoch 52 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693178, and regularization loss is 3.1e-05.\n",
            " Top K precision = 0.0978260869565217, recall = 0.0076102932940919875.\n",
            "Training on epoch 52 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.0814432989690721, recall = 0.008280331949706059.\n",
            "Training on epoch 52 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08924731182795696, recall = 0.00833147272592054.\n",
            "Training on epoch 52 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.0934065934065934, recall = 0.009881777082834216.\n",
            "Training on epoch 52 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09680851063829785, recall = 0.007654874353186602.\n",
            "Training on epoch 52 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.09090909090909087, recall = 0.007890373269745206.\n",
            "Training on epoch 52 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08469387755102038, recall = 0.007972836162271341.\n",
            "\n",
            "Training on 52 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999995, recall = 0.007539975134993573.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08549999999999995, recall = 0.007209174341386591.\n",
            "\n",
            "Training on the 53 epoch\n",
            "Training on epoch 53 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08494623655913977, recall = 0.0073479489216537585.\n",
            "Training on epoch 53 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.10396039603960391, recall = 0.009664145249443486.\n",
            "Training on epoch 53 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.11030927835051538, recall = 0.008912864183336039.\n",
            "Training on epoch 53 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.09789473684210523, recall = 0.00828234818467338.\n",
            "Training on epoch 53 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0949494949494949, recall = 0.007908990147979885.\n",
            "Training on epoch 53 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.08602150537634407, recall = 0.007326337117790522.\n",
            "Training on epoch 53 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09896907216494841, recall = 0.005038388339902775.\n",
            "Training on epoch 53 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.083695652173913, recall = 0.006873548025508338.\n",
            "\n",
            "Training on 53 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 54 epoch\n",
            "Training on epoch 54 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.08817204301075267, recall = 0.0056702562644492605.\n",
            "Training on epoch 54 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0857142857142857, recall = 0.007247893028229472.\n",
            "Training on epoch 54 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.07899999999999996, recall = 0.006220842644388802.\n",
            "Training on epoch 54 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08124999999999996, recall = 0.006864689517012614.\n",
            "Training on epoch 54 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08804347826086953, recall = 0.007898329421799905.\n",
            "Training on epoch 54 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09340659340659337, recall = 0.008250477974892928.\n",
            "Training on epoch 54 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09139784946236557, recall = 0.008092683415598288.\n",
            "Training on epoch 54 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09583333333333331, recall = 0.008203829229845671.\n",
            "\n",
            "Training on 54 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999998, recall = 0.007488282363068663.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 55 epoch\n",
            "Training on epoch 55 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09795918367346933, recall = 0.008214445300680445.\n",
            "Training on epoch 55 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08899999999999995, recall = 0.006273996518778631.\n",
            "Training on epoch 55 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.10425531914893615, recall = 0.008245082437466276.\n",
            "Training on epoch 55 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.105050505050505, recall = 0.008366200544773616.\n",
            "Training on epoch 55 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07961165048543689, recall = 0.006261460165833991.\n",
            "Training on epoch 55 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09793814432989686, recall = 0.008359528207711584.\n",
            "Training on epoch 55 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08666666666666663, recall = 0.0072384446668537755.\n",
            "Training on epoch 55 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08977272727272728, recall = 0.008504807638849698.\n",
            "\n",
            "Training on 55 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999997, recall = 0.007470805546941367.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08349999999999995, recall = 0.0069999989269753935.\n",
            "\n",
            "Training on the 56 epoch\n",
            "Training on epoch 56 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07934782608695652, recall = 0.0074848208379927434.\n",
            "Training on epoch 56 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.0828282828282828, recall = 0.00825918357755953.\n",
            "Training on epoch 56 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09891304347826084, recall = 0.007569148998930676.\n",
            "Training on epoch 56 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09787234042553188, recall = 0.007755506946136587.\n",
            "Training on epoch 56 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08876404494382022, recall = 0.006645757153602586.\n",
            "Training on epoch 56 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.07526881720430106, recall = 0.006959831032949822.\n",
            "Training on epoch 56 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08989898989898988, recall = 0.007670015524967662.\n",
            "Training on epoch 56 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09473684210526313, recall = 0.007467755674473826.\n",
            "\n",
            "Training on 56 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08949999999999994, recall = 0.007690178228935324.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.007525439600505615.\n",
            "\n",
            "Training on the 57 epoch\n",
            "Training on epoch 57 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.07142857142857144, recall = 0.006474991953819021.\n",
            "Training on epoch 57 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08700000000000001, recall = 0.005087389616665648.\n",
            "Training on epoch 57 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08681318681318678, recall = 0.006221034819138334.\n",
            "Training on epoch 57 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09793814432989686, recall = 0.007496631360697404.\n",
            "Training on epoch 57 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08829787234042548, recall = 0.007114302211694463.\n",
            "Training on epoch 57 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.10105263157894735, recall = 0.00805758724804295.\n",
            "Training on epoch 57 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0833333333333333, recall = 0.007145951213653978.\n",
            "Training on epoch 57 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.07959183673469385, recall = 0.006901570073466872.\n",
            "\n",
            "Training on 57 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999997, recall = 0.007470661225219138.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.007567896215066018.\n",
            "\n",
            "Training on the 58 epoch\n",
            "Training on epoch 58 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08191489361702128, recall = 0.006029856494322112.\n",
            "Training on epoch 58 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09479166666666665, recall = 0.007392591739166861.\n",
            "Training on epoch 58 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08478260869565216, recall = 0.008657993169422798.\n",
            "Training on epoch 58 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08586956521739124, recall = 0.00845399436988463.\n",
            "Training on epoch 58 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08631578947368417, recall = 0.009477736640537743.\n",
            "Training on epoch 58 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.05894736842105261, recall = 0.006471690628402895.\n",
            "Training on epoch 58 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0917525773195876, recall = 0.008315253100635979.\n",
            "Training on epoch 58 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09591836734693875, recall = 0.007524474469275302.\n",
            "\n",
            "Training on 58 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 59 epoch\n",
            "Training on epoch 59 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07857142857142856, recall = 0.008339160385758125.\n",
            "Training on epoch 59 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.0868131868131868, recall = 0.00715055524177456.\n",
            "Training on epoch 59 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09462365591397846, recall = 0.009378462116508958.\n",
            "Training on epoch 59 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07777777777777775, recall = 0.007599530859487995.\n",
            "Training on epoch 59 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08367346938775508, recall = 0.0058278417507062604.\n",
            "Training on epoch 59 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09890109890109888, recall = 0.00868448976949433.\n",
            "Training on epoch 59 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08461538461538458, recall = 0.007385501303407235.\n",
            "Training on epoch 59 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08350515463917521, recall = 0.00753664818294854.\n",
            "\n",
            "Training on 59 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08900000000000001, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08849999999999997, recall = 0.007546547188599471.\n",
            "\n",
            "Training on the 60 epoch\n",
            "Training on epoch 60 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08350515463917524, recall = 0.008267543081921642.\n",
            "Training on epoch 60 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09080459770114942, recall = 0.008504408610750908.\n",
            "Training on epoch 60 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09693877551020405, recall = 0.007506236876564798.\n",
            "Training on epoch 60 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.10777777777777771, recall = 0.009605741848151479.\n",
            "Training on epoch 60 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07676767676767675, recall = 0.0067908111053214945.\n",
            "Training on epoch 60 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09368421052631573, recall = 0.00805506792019013.\n",
            "Training on epoch 60 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.091578947368421, recall = 0.0068266410181107694.\n",
            "Training on epoch 60 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07959183673469386, recall = 0.008241323425090272.\n",
            "\n",
            "Training on 60 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08999999999999994, recall = 0.007599257957434288.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999997, recall = 0.007563071975571767.\n",
            "\n",
            "Training on the 61 epoch\n",
            "Training on epoch 61 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08494623655913977, recall = 0.006172525717029719.\n",
            "Training on epoch 61 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09099999999999998, recall = 0.007367652428374623.\n",
            "Training on epoch 61 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.10208333333333329, recall = 0.008747568535613382.\n",
            "Training on epoch 61 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08064516129032254, recall = 0.006323451430709786.\n",
            "Training on epoch 61 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0833333333333333, recall = 0.007021943899343974.\n",
            "Training on epoch 61 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.10947368421052628, recall = 0.00815898380536488.\n",
            "Training on epoch 61 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08817204301075267, recall = 0.006031874906560538.\n",
            "Training on epoch 61 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08910891089108909, recall = 0.007573603249201431.\n",
            "\n",
            "Training on 61 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08850000000000001, recall = 0.007534500282814581.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.007532819031235891.\n",
            "\n",
            "Training on the 62 epoch\n",
            "Training on epoch 62 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09052631578947366, recall = 0.007626124355430437.\n",
            "Training on epoch 62 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.09468085106382976, recall = 0.009419400468812384.\n",
            "Training on epoch 62 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08350515463917521, recall = 0.006583669888036195.\n",
            "Training on epoch 62 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08888888888888888, recall = 0.006541380537408826.\n",
            "Training on epoch 62 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08936170212765958, recall = 0.008005798775385252.\n",
            "Training on epoch 62 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09166666666666662, recall = 0.0071838846684612115.\n",
            "Training on epoch 62 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09687499999999999, recall = 0.007054682845849057.\n",
            "Training on epoch 62 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693179, and regularization loss is 3.2e-05.\n",
            " Top K precision = 0.09999999999999991, recall = 0.007978372125068382.\n",
            "\n",
            "Training on 62 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08649999999999994, recall = 0.0074428450308037354.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007468732352930257.\n",
            "\n",
            "Training on the 63 epoch\n",
            "Training on epoch 63 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09479166666666662, recall = 0.008522634246058106.\n",
            "Training on epoch 63 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09072164948453605, recall = 0.0080822291706784.\n",
            "Training on epoch 63 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10306122448979584, recall = 0.0067751953456281905.\n",
            "Training on epoch 63 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08265306122448979, recall = 0.007071802077530414.\n",
            "Training on epoch 63 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 8e-06.\n",
            " Top K precision = 0.08124999999999999, recall = 0.007418897737628719.\n",
            "Training on epoch 63 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.06770833333333331, recall = 0.0075862604151711865.\n",
            "Training on epoch 63 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09595959595959591, recall = 0.007230876904591842.\n",
            "Training on epoch 63 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.07473684210526314, recall = 0.0062292677598452615.\n",
            "\n",
            "Training on 63 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007558772127474779.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08849999999999994, recall = 0.007536450698903348.\n",
            "\n",
            "Training on the 64 epoch\n",
            "Training on epoch 64 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09374999999999994, recall = 0.007850405676628478.\n",
            "Training on epoch 64 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08681318681318681, recall = 0.007096967231008651.\n",
            "Training on epoch 64 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69318, and regularization loss is 3.3e-05.\n",
            " Top K precision = 0.09456521739130432, recall = 0.008649973488535776.\n",
            "Training on epoch 64 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09239130434782607, recall = 0.0071838310383879386.\n",
            "Training on epoch 64 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0901960784313725, recall = 0.007954364736091252.\n",
            "Training on epoch 64 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.07843137254901959, recall = 0.006931874413395164.\n",
            "Training on epoch 64 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09247311827956989, recall = 0.006423765539212816.\n",
            "Training on epoch 64 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693174, and regularization loss is 2.7e-05.\n",
            " Top K precision = 0.09693877551020402, recall = 0.008028586513934018.\n",
            "\n",
            "Training on 64 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474777.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007437587649848244.\n",
            "\n",
            "Training on the 65 epoch\n",
            "Training on epoch 65 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09354838709677417, recall = 0.00822851533105163.\n",
            "Training on epoch 65 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.08085106382978723, recall = 0.007880899038030922.\n",
            "Training on epoch 65 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0723404255319149, recall = 0.005836203465437944.\n",
            "Training on epoch 65 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.07684210526315788, recall = 0.006217793802187964.\n",
            "Training on epoch 65 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08421052631578946, recall = 0.006040502726352921.\n",
            "Training on epoch 65 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.09191919191919186, recall = 0.006794870578460744.\n",
            "Training on epoch 65 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.06989247311827956, recall = 0.005504376319695413.\n",
            "Training on epoch 65 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08913043478260867, recall = 0.0067862303944285825.\n",
            "\n",
            "Training on 65 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007558772127474772.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.007450704347469689.\n",
            "\n",
            "Training on the 66 epoch\n",
            "Training on epoch 66 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.0945652173913043, recall = 0.008523054880675372.\n",
            "Training on epoch 66 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0948979591836734, recall = 0.00785179091450278.\n",
            "Training on epoch 66 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08888888888888882, recall = 0.007537419460765351.\n",
            "Training on epoch 66 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.06263736263736264, recall = 0.0056823623052046435.\n",
            "Training on epoch 66 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.07872340425531915, recall = 0.00654300763959299.\n",
            "Training on epoch 66 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08499999999999998, recall = 0.007477878269187234.\n",
            "Training on epoch 66 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08229166666666665, recall = 0.0070451886399108985.\n",
            "Training on epoch 66 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0894736842105263, recall = 0.00806936469072109.\n",
            "\n",
            "Training on 66 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474777.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007466013117477004.\n",
            "\n",
            "Training on the 67 epoch\n",
            "Training on epoch 67 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09479166666666665, recall = 0.009126028894575302.\n",
            "Training on epoch 67 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0690721649484536, recall = 0.007268778448042523.\n",
            "Training on epoch 67 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09374999999999996, recall = 0.008667677599139749.\n",
            "Training on epoch 67 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.0863157894736842, recall = 0.006698184109855729.\n",
            "Training on epoch 67 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.0804347826086956, recall = 0.007139076063627265.\n",
            "Training on epoch 67 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.08478260869565216, recall = 0.007066966117347958.\n",
            "Training on epoch 67 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10888888888888884, recall = 0.008345345262893829.\n",
            "Training on epoch 67 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0887755102040816, recall = 0.007954733310061593.\n",
            "\n",
            "Training on 67 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007536249604952253.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999997, recall = 0.007449515091571408.\n",
            "\n",
            "Training on the 68 epoch\n",
            "Training on epoch 68 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.0912087912087912, recall = 0.008327255937444497.\n",
            "Training on epoch 68 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08390804597701149, recall = 0.006513153147925869.\n",
            "Training on epoch 68 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08842105263157891, recall = 0.007514813956022416.\n",
            "Training on epoch 68 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08620689655172413, recall = 0.0068781121436088406.\n",
            "Training on epoch 68 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.10430107526881714, recall = 0.009758706321909658.\n",
            "Training on epoch 68 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09393939393939388, recall = 0.008131791529849377.\n",
            "Training on epoch 68 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.07799999999999999, recall = 0.006172542862989718.\n",
            "Training on epoch 68 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693179, and regularization loss is 3.2e-05.\n",
            " Top K precision = 0.0897959183673469, recall = 0.008195218768648725.\n",
            "\n",
            "Training on 68 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08849999999999997, recall = 0.007628661872365674.\n",
            "\n",
            "Training on the 69 epoch\n",
            "Training on epoch 69 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09405940594059405, recall = 0.007917768528511735.\n",
            "Training on epoch 69 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08617021276595742, recall = 0.007798699563012491.\n",
            "Training on epoch 69 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08469387755102038, recall = 0.0059999238801802125.\n",
            "Training on epoch 69 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.10235294117647058, recall = 0.008333237536644651.\n",
            "Training on epoch 69 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.09230769230769226, recall = 0.0072048590713791934.\n",
            "Training on epoch 69 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09010989010989008, recall = 0.007886327994701707.\n",
            "Training on epoch 69 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.06979166666666664, recall = 0.006770475739406644.\n",
            "Training on epoch 69 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.0792079207920792, recall = 0.0067097787407191965.\n",
            "\n",
            "Training on 69 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.0075477658381666.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08549999999999995, recall = 0.007379584337131873.\n",
            "\n",
            "Training on the 70 epoch\n",
            "Training on epoch 70 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693182, and regularization loss is 3.5e-05.\n",
            " Top K precision = 0.0887755102040816, recall = 0.008710571230380491.\n",
            "Training on epoch 70 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08229166666666662, recall = 0.007208390849306898.\n",
            "Training on epoch 70 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08921568627450978, recall = 0.007141810596837529.\n",
            "Training on epoch 70 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08571428571428572, recall = 0.0064303462589405305.\n",
            "Training on epoch 70 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09578947368421051, recall = 0.009144037359703658.\n",
            "Training on epoch 70 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0946236559139785, recall = 0.007747925280340101.\n",
            "Training on epoch 70 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.09885057471264365, recall = 0.007236178608711838.\n",
            "Training on epoch 70 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.07553191489361702, recall = 0.008248044617568475.\n",
            "\n",
            "Training on 70 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.0075450358637385116.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08999999999999997, recall = 0.007649420112558152.\n",
            "\n",
            "Training on the 71 epoch\n",
            "Training on epoch 71 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09222222222222218, recall = 0.00813876896948504.\n",
            "Training on epoch 71 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1e-05.\n",
            " Top K precision = 0.0887755102040816, recall = 0.007186553549302621.\n",
            "Training on epoch 71 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0897959183673469, recall = 0.008850532859738695.\n",
            "Training on epoch 71 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.07959183673469386, recall = 0.0062145992495480544.\n",
            "Training on epoch 71 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08979591836734689, recall = 0.006956555784764356.\n",
            "Training on epoch 71 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0845360824742268, recall = 0.005944393720907629.\n",
            "Training on epoch 71 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08247422680412371, recall = 0.00742812354156734.\n",
            "Training on epoch 71 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.08817204301075267, recall = 0.008482690611347932.\n",
            "\n",
            "Training on 71 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474773.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 72 epoch\n",
            "Training on epoch 72 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.07755102040816327, recall = 0.006481878249850415.\n",
            "Training on epoch 72 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08152173913043477, recall = 0.009010211779368633.\n",
            "Training on epoch 72 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09499999999999996, recall = 0.00738687036747555.\n",
            "Training on epoch 72 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09247311827956989, recall = 0.006487689176803379.\n",
            "Training on epoch 72 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.06702127659574465, recall = 0.008252396589031846.\n",
            "Training on epoch 72 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08105263157894736, recall = 0.007839805188281947.\n",
            "Training on epoch 72 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.08659793814432988, recall = 0.006691564534336313.\n",
            "Training on epoch 72 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09565217391304341, recall = 0.006785341763043015.\n",
            "\n",
            "Training on 72 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999995, recall = 0.007569656957221465.\n",
            "\n",
            "Training on the 73 epoch\n",
            "Training on epoch 73 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09892473118279566, recall = 0.008736338843432487.\n",
            "Training on epoch 73 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0939393939393939, recall = 0.0073461956309617505.\n",
            "Training on epoch 73 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07653061224489797, recall = 0.006548782220845298.\n",
            "Training on epoch 73 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.07959183673469383, recall = 0.006949607001864963.\n",
            "Training on epoch 73 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08080808080808077, recall = 0.006428658506831454.\n",
            "Training on epoch 73 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.07475728155339803, recall = 0.007305865822355697.\n",
            "Training on epoch 73 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.6e-05.\n",
            " Top K precision = 0.10109890109890109, recall = 0.009060598545504082.\n",
            "Training on epoch 73 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693154, and regularization loss is 7e-06.\n",
            " Top K precision = 0.09797979797979793, recall = 0.008754503196457564.\n",
            "\n",
            "Training on 73 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007488349592263508.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08849999999999994, recall = 0.007611272087713941.\n",
            "\n",
            "Training on the 74 epoch\n",
            "Training on epoch 74 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09578947368421048, recall = 0.008046964589454045.\n",
            "Training on epoch 74 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09052631578947365, recall = 0.007665941341720766.\n",
            "Training on epoch 74 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.083695652173913, recall = 0.006914126242503714.\n",
            "Training on epoch 74 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09032258064516122, recall = 0.006396957781202903.\n",
            "Training on epoch 74 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09684210526315787, recall = 0.007250648639214636.\n",
            "Training on epoch 74 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.10208333333333329, recall = 0.00913788275141057.\n",
            "Training on epoch 74 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08617021276595742, recall = 0.007328253984795101.\n",
            "Training on epoch 74 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10109890109890106, recall = 0.009629623083779518.\n",
            "\n",
            "Training on 74 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007545918657037759.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08599999999999997, recall = 0.00739180927600718.\n",
            "\n",
            "Training on the 75 epoch\n",
            "Training on epoch 75 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08811881188118807, recall = 0.009608598352892458.\n",
            "Training on epoch 75 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08775510204081631, recall = 0.008048620814285798.\n",
            "Training on epoch 75 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08484848484848484, recall = 0.00766552798500829.\n",
            "Training on epoch 75 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.0989583333333333, recall = 0.008829501799570948.\n",
            "Training on epoch 75 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0938144329896907, recall = 0.006855824261761646.\n",
            "Training on epoch 75 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0912087912087912, recall = 0.006202522844227843.\n",
            "Training on epoch 75 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09591836734693875, recall = 0.007986895762707237.\n",
            "Training on epoch 75 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.0917525773195876, recall = 0.006136613391795623.\n",
            "\n",
            "Training on 75 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007468732352930257.\n",
            "\n",
            "Training on the 76 epoch\n",
            "Training on epoch 76 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.07526881720430104, recall = 0.006056289800433048.\n",
            "Training on epoch 76 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08686868686868683, recall = 0.007961466820197447.\n",
            "Training on epoch 76 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08799999999999995, recall = 0.0069604224620151865.\n",
            "Training on epoch 76 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08, recall = 0.007116562398499759.\n",
            "Training on epoch 76 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.0918367346938775, recall = 0.006711914957967092.\n",
            "Training on epoch 76 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08681318681318678, recall = 0.004699878537678107.\n",
            "Training on epoch 76 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09479166666666665, recall = 0.007649093844925569.\n",
            "Training on epoch 76 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08571428571428569, recall = 0.008200497553639755.\n",
            "\n",
            "Training on 76 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08699999999999992, recall = 0.007387559520705358.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08499999999999995, recall = 0.007099138799714572.\n",
            "\n",
            "Training on the 77 epoch\n",
            "Training on epoch 77 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08039215686274509, recall = 0.007141689529086974.\n",
            "Training on epoch 77 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.10210526315789471, recall = 0.008157430287237775.\n",
            "Training on epoch 77 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09890109890109888, recall = 0.008855468346641788.\n",
            "Training on epoch 77 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09230769230769227, recall = 0.006582049247777658.\n",
            "Training on epoch 77 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07701149425287357, recall = 0.006060117071050772.\n",
            "Training on epoch 77 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09784946236559139, recall = 0.0071955293512950096.\n",
            "Training on epoch 77 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08749999999999998, recall = 0.008650665673753615.\n",
            "Training on epoch 77 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.0927083333333333, recall = 0.006478988936161063.\n",
            "\n",
            "Training on 77 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.007653111750116286.\n",
            "\n",
            "Training on the 78 epoch\n",
            "Training on epoch 78 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.07849462365591395, recall = 0.007541709506783622.\n",
            "Training on epoch 78 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.07752808988764043, recall = 0.006478660386449764.\n",
            "Training on epoch 78 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09793814432989686, recall = 0.006862965735594071.\n",
            "Training on epoch 78 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08865979381443295, recall = 0.006971081150117919.\n",
            "Training on epoch 78 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08061224489795918, recall = 0.007429248228242597.\n",
            "Training on epoch 78 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.08787878787878785, recall = 0.006886223853963773.\n",
            "Training on epoch 78 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09183673469387753, recall = 0.007358073610038315.\n",
            "Training on epoch 78 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.08645833333333332, recall = 0.008123362503940404.\n",
            "\n",
            "Training on 78 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 79 epoch\n",
            "Training on epoch 79 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08404255319148932, recall = 0.007911404925370213.\n",
            "Training on epoch 79 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08617021276595742, recall = 0.008401874611660773.\n",
            "Training on epoch 79 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08316831683168312, recall = 0.00706705179059652.\n",
            "Training on epoch 79 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07938144329896907, recall = 0.007196876336668509.\n",
            "Training on epoch 79 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08762886597938142, recall = 0.007270325684534053.\n",
            "Training on epoch 79 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07999999999999997, recall = 0.008139383766290979.\n",
            "Training on epoch 79 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08399999999999996, recall = 0.0088944065087861.\n",
            "Training on epoch 79 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.075, recall = 0.006990831881048736.\n",
            "\n",
            "Training on 79 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 80 epoch\n",
            "Training on epoch 80 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.0912087912087912, recall = 0.007449743377279628.\n",
            "Training on epoch 80 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09680851063829785, recall = 0.008348141847770397.\n",
            "Training on epoch 80 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07526881720430105, recall = 0.007979661863134514.\n",
            "Training on epoch 80 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08762886597938141, recall = 0.00695742860321351.\n",
            "Training on epoch 80 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09270833333333328, recall = 0.0061665501621531275.\n",
            "Training on epoch 80 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09207920792079209, recall = 0.007335920098841247.\n",
            "Training on epoch 80 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09263157894736841, recall = 0.006235429873800233.\n",
            "Training on epoch 80 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08426966292134827, recall = 0.00662807837241857.\n",
            "\n",
            "Training on 80 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007445135763838411.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007499767212330569.\n",
            "\n",
            "Training on the 81 epoch\n",
            "Training on epoch 81 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09899999999999995, recall = 0.007728883421457595.\n",
            "Training on epoch 81 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08279569892473114, recall = 0.007414277723606067.\n",
            "Training on epoch 81 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09010989010989008, recall = 0.008942468846849282.\n",
            "Training on epoch 81 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.9e-05.\n",
            " Top K precision = 0.09892473118279563, recall = 0.008191046483562617.\n",
            "Training on epoch 81 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09680851063829783, recall = 0.00871496435721655.\n",
            "Training on epoch 81 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0894736842105263, recall = 0.00749181709836023.\n",
            "Training on epoch 81 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.10927835051546389, recall = 0.010538938052056634.\n",
            "Training on epoch 81 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 9e-06.\n",
            " Top K precision = 0.07395833333333329, recall = 0.007726277088498085.\n",
            "\n",
            "Training on 81 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999997, recall = 0.0075180328716451805.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.007653111750116286.\n",
            "\n",
            "Training on the 82 epoch\n",
            "Training on epoch 82 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09599999999999995, recall = 0.006473305644401222.\n",
            "Training on epoch 82 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.09896907216494844, recall = 0.008209164909560761.\n",
            "Training on epoch 82 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.07752808988764041, recall = 0.008485979439483626.\n",
            "Training on epoch 82 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08478260869565216, recall = 0.007504281696686233.\n",
            "Training on epoch 82 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.09999999999999995, recall = 0.008395654560181097.\n",
            "Training on epoch 82 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08969072164948451, recall = 0.007062457366857315.\n",
            "Training on epoch 82 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09780219780219777, recall = 0.009439207690291728.\n",
            "Training on epoch 82 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.06304347826086956, recall = 0.006658567631188078.\n",
            "\n",
            "Training on 82 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.007528284322596728.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999997, recall = 0.007493671843220101.\n",
            "\n",
            "Training on the 83 epoch\n",
            "Training on epoch 83 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08736842105263155, recall = 0.007110611502755667.\n",
            "Training on epoch 83 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08241758241758237, recall = 0.006668245541159611.\n",
            "Training on epoch 83 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.0947916666666666, recall = 0.006952018447128827.\n",
            "Training on epoch 83 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08709677419354839, recall = 0.0073080256852134555.\n",
            "Training on epoch 83 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08541666666666663, recall = 0.00836977814548233.\n",
            "Training on epoch 83 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09603960396039601, recall = 0.007988108983828767.\n",
            "Training on epoch 83 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.11428571428571423, recall = 0.00943112211361096.\n",
            "Training on epoch 83 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.079, recall = 0.006620021819251925.\n",
            "\n",
            "Training on 83 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999994, recall = 0.007534322249724165.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.007653111750116286.\n",
            "\n",
            "Training on the 84 epoch\n",
            "Training on epoch 84 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.08383838383838381, recall = 0.007739871747876855.\n",
            "Training on epoch 84 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09062499999999996, recall = 0.0068947798858781835.\n",
            "Training on epoch 84 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08979591836734689, recall = 0.006934347514982463.\n",
            "Training on epoch 84 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.08041237113402062, recall = 0.007660784560121389.\n",
            "Training on epoch 84 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.07835051546391751, recall = 0.007263071929216001.\n",
            "Training on epoch 84 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08799999999999995, recall = 0.0072260878908973.\n",
            "Training on epoch 84 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09263157894736837, recall = 0.007946715007010018.\n",
            "Training on epoch 84 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08181818181818179, recall = 0.007845271662638943.\n",
            "\n",
            "Training on 84 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999998, recall = 0.007558772127474775.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08599999999999997, recall = 0.007497971691317091.\n",
            "\n",
            "Training on the 85 epoch\n",
            "Training on epoch 85 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.10526315789473681, recall = 0.00667240895644938.\n",
            "Training on epoch 85 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.09081632653061221, recall = 0.007525889803768747.\n",
            "Training on epoch 85 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08804347826086956, recall = 0.007793229990224374.\n",
            "Training on epoch 85 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09999999999999999, recall = 0.007749799481298699.\n",
            "Training on epoch 85 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08555555555555552, recall = 0.007215851843800148.\n",
            "Training on epoch 85 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10212765957446804, recall = 0.00770558814910781.\n",
            "Training on epoch 85 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693172, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.08260869565217392, recall = 0.00601116863245603.\n",
            "Training on epoch 85 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.07717391304347823, recall = 0.008317256327971341.\n",
            "\n",
            "Training on 85 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08699999999999995, recall = 0.007553566272149714.\n",
            "\n",
            "Training on the 86 epoch\n",
            "Training on epoch 86 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.11333333333333327, recall = 0.008857880779065144.\n",
            "Training on epoch 86 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.08556701030927834, recall = 0.007095486693566354.\n",
            "Training on epoch 86 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09899999999999996, recall = 0.007924882177191738.\n",
            "Training on epoch 86 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.0864583333333333, recall = 0.007200716770407037.\n",
            "Training on epoch 86 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08124999999999999, recall = 0.005837615464915762.\n",
            "Training on epoch 86 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.10792079207920789, recall = 0.008461819832047891.\n",
            "Training on epoch 86 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08936170212765956, recall = 0.006934249672370116.\n",
            "Training on epoch 86 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08369565217391299, recall = 0.00676914236036165.\n",
            "\n",
            "Training on 86 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999997, recall = 0.007558772127474777.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999997, recall = 0.00748089780305191.\n",
            "\n",
            "Training on the 87 epoch\n",
            "Training on epoch 87 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09299999999999996, recall = 0.009096247353208839.\n",
            "Training on epoch 87 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.09999999999999996, recall = 0.007752088946505499.\n",
            "Training on epoch 87 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08247422680412371, recall = 0.006432608406936436.\n",
            "Training on epoch 87 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09791666666666661, recall = 0.008542281316919843.\n",
            "Training on epoch 87 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08160919540229886, recall = 0.00805572649414207.\n",
            "Training on epoch 87 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.0742268041237113, recall = 0.006702466570023481.\n",
            "Training on epoch 87 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09456521739130429, recall = 0.009771317382266047.\n",
            "Training on epoch 87 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.108695652173913, recall = 0.008014723875512945.\n",
            "\n",
            "Training on 87 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999991, recall = 0.00763633086691476.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08749999999999995, recall = 0.007517439292059241.\n",
            "\n",
            "Training on the 88 epoch\n",
            "Training on epoch 88 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.09347826086956518, recall = 0.008052844568014009.\n",
            "Training on epoch 88 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693173, and regularization loss is 2.5e-05.\n",
            " Top K precision = 0.08723404255319146, recall = 0.007613929372655628.\n",
            "Training on epoch 88 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.0797752808988764, recall = 0.0058714048917714685.\n",
            "Training on epoch 88 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.08068181818181817, recall = 0.007470796631777569.\n",
            "Training on epoch 88 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.10099999999999994, recall = 0.008250138046804236.\n",
            "Training on epoch 88 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08469387755102038, recall = 0.00782511347769141.\n",
            "Training on epoch 88 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693182, and regularization loss is 3.5e-05.\n",
            " Top K precision = 0.09999999999999995, recall = 0.00748188647648997.\n",
            "Training on epoch 88 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.08787878787878782, recall = 0.0074332352994104685.\n",
            "\n",
            "Training on 88 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999991, recall = 0.007530255017208616.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 89 epoch\n",
            "Training on epoch 89 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 8e-06.\n",
            " Top K precision = 0.09569892473118276, recall = 0.009810962144442882.\n",
            "Training on epoch 89 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08749999999999997, recall = 0.006415559622672641.\n",
            "Training on epoch 89 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09793814432989686, recall = 0.00738881390288221.\n",
            "Training on epoch 89 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09090909090909084, recall = 0.00794874681732186.\n",
            "Training on epoch 89 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08791208791208789, recall = 0.007446415092996252.\n",
            "Training on epoch 89 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08947368421052626, recall = 0.006719402353134675.\n",
            "Training on epoch 89 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.4e-05.\n",
            " Top K precision = 0.10210526315789467, recall = 0.007845984385726283.\n",
            "Training on epoch 89 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.0793103448275862, recall = 0.006960400561821854.\n",
            "\n",
            "Training on 89 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08749999999999997, recall = 0.007498623196314736.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08649999999999995, recall = 0.007459116805826504.\n",
            "\n",
            "Training on the 90 epoch\n",
            "Training on epoch 90 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09891304347826083, recall = 0.00862182029469306.\n",
            "Training on epoch 90 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09062499999999996, recall = 0.00745617251969767.\n",
            "Training on epoch 90 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08105263157894733, recall = 0.005778641739341392.\n",
            "Training on epoch 90 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10638297872340421, recall = 0.007199607748541024.\n",
            "Training on epoch 90 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08989898989898987, recall = 0.00845255318566255.\n",
            "Training on epoch 90 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0872549019607843, recall = 0.007670739191785991.\n",
            "Training on epoch 90 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.08723404255319146, recall = 0.007494351080464348.\n",
            "Training on epoch 90 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0783505154639175, recall = 0.006585409241399236.\n",
            "\n",
            "Training on 90 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474776.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.0075682778308968305.\n",
            "\n",
            "Training on the 91 epoch\n",
            "Training on epoch 91 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.09784946236559137, recall = 0.007420639358421715.\n",
            "Training on epoch 91 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.09255319148936166, recall = 0.007962352005326277.\n",
            "Training on epoch 91 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08659793814432987, recall = 0.006349780150474868.\n",
            "Training on epoch 91 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09784946236559136, recall = 0.007876322151337834.\n",
            "Training on epoch 91 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.06489361702127659, recall = 0.006994853489582751.\n",
            "Training on epoch 91 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.07843137254901959, recall = 0.008275909412524207.\n",
            "Training on epoch 91 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.08723404255319146, recall = 0.008612936300848247.\n",
            "Training on epoch 91 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.10109890109890107, recall = 0.007879349087968794.\n",
            "\n",
            "Training on 91 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999995, recall = 0.007489822784135189.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08549999999999995, recall = 0.007433716214560331.\n",
            "\n",
            "Training on the 92 epoch\n",
            "Training on epoch 92 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.07551020408163263, recall = 0.007271494665645925.\n",
            "Training on epoch 92 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0762886597938144, recall = 0.007230030646570279.\n",
            "Training on epoch 92 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07878787878787875, recall = 0.0057447797161090395.\n",
            "Training on epoch 92 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.0797752808988764, recall = 0.007530648173009561.\n",
            "Training on epoch 92 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08969072164948452, recall = 0.0060443457907795135.\n",
            "Training on epoch 92 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09619047619047616, recall = 0.008351959089216394.\n",
            "Training on epoch 92 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0923076923076923, recall = 0.008831063598367283.\n",
            "Training on epoch 92 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.0757894736842105, recall = 0.005694739244606657.\n",
            "\n",
            "Training on 92 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999998, recall = 0.0075417653247536855.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08449999999999994, recall = 0.007384620661831474.\n",
            "\n",
            "Training on the 93 epoch\n",
            "Training on epoch 93 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08299999999999995, recall = 0.006803619827184428.\n",
            "Training on epoch 93 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.07526881720430105, recall = 0.007986040951270307.\n",
            "Training on epoch 93 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693176, and regularization loss is 2.9e-05.\n",
            " Top K precision = 0.09595959595959591, recall = 0.008823823287896592.\n",
            "Training on epoch 93 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693177, and regularization loss is 3e-05.\n",
            " Top K precision = 0.09677419354838708, recall = 0.007056126794067965.\n",
            "Training on epoch 93 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693155, and regularization loss is 7e-06.\n",
            " Top K precision = 0.0782178217821782, recall = 0.008005764698177141.\n",
            "Training on epoch 93 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08275862068965516, recall = 0.008797346874077688.\n",
            "Training on epoch 93 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.10689655172413792, recall = 0.008430273948898993.\n",
            "Training on epoch 93 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693179, and regularization loss is 3.2e-05.\n",
            " Top K precision = 0.09361702127659571, recall = 0.008247243814254067.\n",
            "\n",
            "Training on 93 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08799999999999991, recall = 0.0075080856501379325.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.0074818490505517.\n",
            "\n",
            "Training on the 94 epoch\n",
            "Training on epoch 94 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09999999999999999, recall = 0.008193743261005837.\n",
            "Training on epoch 94 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.07578947368421052, recall = 0.007358357765951169.\n",
            "Training on epoch 94 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693171, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.0891304347826087, recall = 0.007880730832042725.\n",
            "Training on epoch 94 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09021739130434778, recall = 0.008897465312055075.\n",
            "Training on epoch 94 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.07812499999999999, recall = 0.00697465390786478.\n",
            "Training on epoch 94 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.08681318681318678, recall = 0.007520755857127048.\n",
            "Training on epoch 94 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.08333333333333333, recall = 0.007256947963604103.\n",
            "Training on epoch 94 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.09062499999999996, recall = 0.007897505189533407.\n",
            "\n",
            "Training on 94 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08949999999999997, recall = 0.007777426908911551.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08599999999999994, recall = 0.007479632026341594.\n",
            "\n",
            "Training on the 95 epoch\n",
            "Training on epoch 95 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.0855555555555555, recall = 0.007668677974277297.\n",
            "Training on epoch 95 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693175, and regularization loss is 2.8e-05.\n",
            " Top K precision = 0.0967741935483871, recall = 0.007678448127534203.\n",
            "Training on epoch 95 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08834951456310676, recall = 0.006434575887323016.\n",
            "Training on epoch 95 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.3e-05.\n",
            " Top K precision = 0.09673913043478255, recall = 0.008237426922736029.\n",
            "Training on epoch 95 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693169, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09195402298850575, recall = 0.009335497210129386.\n",
            "Training on epoch 95 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09677419354838705, recall = 0.008003213211392888.\n",
            "Training on epoch 95 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09906542056074763, recall = 0.007070897151701332.\n",
            "Training on epoch 95 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693167, and regularization loss is 2e-05.\n",
            " Top K precision = 0.09587628865979377, recall = 0.0077280894080894525.\n",
            "\n",
            "Training on 95 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007492982653790563.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08299999999999993, recall = 0.007277211336736898.\n",
            "\n",
            "Training on the 96 epoch\n",
            "Training on epoch 96 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.09239130434782605, recall = 0.006690302276700393.\n",
            "Training on epoch 96 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08829787234042552, recall = 0.005889583853701018.\n",
            "Training on epoch 96 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.08350515463917524, recall = 0.008186018769751304.\n",
            "Training on epoch 96 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.08555555555555552, recall = 0.005885020040575276.\n",
            "Training on epoch 96 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08631578947368418, recall = 0.007399470387948645.\n",
            "Training on epoch 96 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.0896907216494845, recall = 0.007784670886978344.\n",
            "Training on epoch 96 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08163265306122447, recall = 0.007995291854727488.\n",
            "Training on epoch 96 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08555555555555552, recall = 0.007513748340932178.\n",
            "\n",
            "Training on 96 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999993, recall = 0.007558772127474779.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08899999999999995, recall = 0.0075587721274747765.\n",
            "\n",
            "Training on the 97 epoch\n",
            "Training on epoch 97 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08020833333333331, recall = 0.008273499168706767.\n",
            "Training on epoch 97 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.69316, and regularization loss is 1.3e-05.\n",
            " Top K precision = 0.08333333333333331, recall = 0.005944253263947247.\n",
            "Training on epoch 97 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.08631578947368417, recall = 0.00839534877025498.\n",
            "Training on epoch 97 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.9e-05.\n",
            " Top K precision = 0.0776595744680851, recall = 0.005836375402159166.\n",
            "Training on epoch 97 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693179, and regularization loss is 3.2e-05.\n",
            " Top K precision = 0.08421052631578944, recall = 0.007972455694494994.\n",
            "Training on epoch 97 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693157, and regularization loss is 1e-05.\n",
            " Top K precision = 0.10109890109890107, recall = 0.007748273443140191.\n",
            "Training on epoch 97 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09462365591397845, recall = 0.0075060838958705516.\n",
            "Training on epoch 97 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.0702127659574468, recall = 0.006969027387886939.\n",
            "\n",
            "Training on 97 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999994, recall = 0.00752031058901324.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.007653111750116286.\n",
            "\n",
            "Training on the 98 epoch\n",
            "Training on epoch 98 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.10099999999999994, recall = 0.008027231564062962.\n",
            "Training on epoch 98 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.0950980392156862, recall = 0.008214251798389796.\n",
            "Training on epoch 98 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.69317, and regularization loss is 2.2e-05.\n",
            " Top K precision = 0.08279569892473114, recall = 0.007680584167342782.\n",
            "Training on epoch 98 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.08124999999999998, recall = 0.007719628325706499.\n",
            "Training on epoch 98 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.5e-05.\n",
            " Top K precision = 0.09175257731958764, recall = 0.0077698524025460285.\n",
            "Training on epoch 98 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693159, and regularization loss is 1.2e-05.\n",
            " Top K precision = 0.08901098901098897, recall = 0.00705882760629446.\n",
            "Training on epoch 98 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693156, and regularization loss is 9e-06.\n",
            " Top K precision = 0.09072164948453604, recall = 0.008380338918207466.\n",
            "Training on epoch 98 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693168, and regularization loss is 2.1e-05.\n",
            " Top K precision = 0.09893617021276593, recall = 0.00632320379183976.\n",
            "\n",
            "Training on 98 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08849999999999997, recall = 0.007545918657037758.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08949999999999995, recall = 0.007653111750116286.\n",
            "\n",
            "Training on the 99 epoch\n",
            "Training on epoch 99 minibatch 1/782 completed\n",
            " bpr_loss on current minibatch is 0.693158, and regularization loss is 1.1e-05.\n",
            " Top K precision = 0.08631578947368418, recall = 0.007548285093418527.\n",
            "Training on epoch 99 minibatch 101/782 completed\n",
            " bpr_loss on current minibatch is 0.693162, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.09468085106382977, recall = 0.007152558973856723.\n",
            "Training on epoch 99 minibatch 201/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.07628865979381441, recall = 0.006644121780606732.\n",
            "Training on epoch 99 minibatch 301/782 completed\n",
            " bpr_loss on current minibatch is 0.693163, and regularization loss is 1.6e-05.\n",
            " Top K precision = 0.09892473118279566, recall = 0.009282384010439074.\n",
            "Training on epoch 99 minibatch 401/782 completed\n",
            " bpr_loss on current minibatch is 0.693164, and regularization loss is 1.7e-05.\n",
            " Top K precision = 0.09787234042553189, recall = 0.00736166354824821.\n",
            "Training on epoch 99 minibatch 501/782 completed\n",
            " bpr_loss on current minibatch is 0.693165, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.08736842105263155, recall = 0.007645911702745472.\n",
            "Training on epoch 99 minibatch 601/782 completed\n",
            " bpr_loss on current minibatch is 0.693166, and regularization loss is 1.8e-05.\n",
            " Top K precision = 0.07604166666666663, recall = 0.007729876612269424.\n",
            "Training on epoch 99 minibatch 701/782 completed\n",
            " bpr_loss on current minibatch is 0.693161, and regularization loss is 1.4e-05.\n",
            " Top K precision = 0.10937499999999999, recall = 0.00913956165753348.\n",
            "\n",
            "Training on 99 epoch completed.\n",
            " Average bpr_loss on train set is 0.005421 for the current epoch.\n",
            " Training top K precision = 0.08899999999999995, recall = 0.007558772127474775.\n",
            " Average bpr_loss on the validation set is 7e-06, and regularization loss is 0.0.\n",
            " Validation top K precision = 0.08799999999999995, recall = 0.0074818490505517.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot top K precision over epochs"
      ],
      "metadata": {
        "id": "77xV_pCdAqV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs_tracked, [precision for precision, _ in train_topks],\n",
        "         label=\"Train\")\n",
        "plt.plot(epochs_tracked, [precision for precision, _ in val_topks],\n",
        "         label=\"Val\")\n",
        "plt.ylabel(f\"Top {K} precision\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "DhWgVYTn6F05",
        "outputId": "a6442b24-8075-48a6-c42c-601c3d3bb5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot top K recall over epochs\n"
      ],
      "metadata": {
        "id": "LRyN5RWZAtJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs_tracked, [recall for _, recall in train_topks],\n",
        "         label=\"Train\")\n",
        "plt.plot(epochs_tracked, [recall for _, recall in val_topks],\n",
        "         label=\"Val\")\n",
        "plt.ylabel(f\"Top {K} recall\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ryhaVfnL9cf3",
        "outputId": "1ae19764-2769-4b3e-dc15-21b3f0b5f727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "\n",
        "After training, let's test our model on the test set."
      ],
      "metadata": {
        "id": "b7odzij5Av3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on the test set\n",
        "lightGCN.eval()\n",
        "print(\"Training completed after {} epochs\".format(epochs))\n",
        "\n",
        "users_test = samples_test[:, 0:1]\n",
        "pos_test = samples_test[:, 1:2]\n",
        "neg_test = samples_test[:, 2:3]\n",
        "\n",
        "loss_test, reg_loss_test = bpr_loss(\n",
        "    lightGCN, users_test, pos_test, neg_test, data, test_mask)\n",
        "reg_loss_test = reg_loss_test * weight_decay\n",
        "\n",
        "# predict on the test set\n",
        "user_indices = samples_test[:, 0]\n",
        "user_indices = user_indices.repeat(2).long()\n",
        "item_indices = torch.cat((samples_test[:, 1], samples_test[:, 2])).long()\n",
        "pred_test = getUsersRating(lightGCN, users_test[:,0], data)\\\n",
        "    [user_indices, item_indices]\n",
        "truth_test = data[\"edge_index\"][users_test.long()[:,0]]\\\n",
        "    [user_indices, item_indices]\n",
        "test_topk_precision, test_topk_recall = personalized_topk(\n",
        "    pred_test, K, user_indices, data[\"edge_index\"])\n",
        "\n",
        "print(\"Average bpr_loss on the test set is {}, and regularization loss is {}.\\n\".format(round(float((loss_test+reg_loss_test)/len(samples_test)), 6),\n",
        "                                                                                                round(float(reg_loss_test/len(samples_test)), 6)),\n",
        "      \"Top K precision = {}, recall = {}.\".format(test_topk_precision, test_topk_recall))\n",
        "\n",
        "# Save model embeddings.\n",
        "torch.save(lightGCN, config_dict[\"model_name\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJnetNBG6C_k",
        "outputId": "c27552bc-a116-4c68-9c81-8bdf7d8e64b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed after 100 epochs\n",
            "Average bpr_loss on the test set is 7e-06, and regularization loss is 0.0.\n",
            " Top K precision = 0.08849999999999997, recall = 0.007536249604952254.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run matrix factorization as baseline performance\n",
        "\n",
        "As a baseline, we can run [PARAFAC matrix factorization](https://www.sciencedirect.com/science/article/abs/pii/S0169743997000324) on the dataset. We use the [TensorLy](http://tensorly.org/dev/modules/generated/tensorly.decomposition.parafac.html#tensorly.decomposition.parafac) library."
      ],
      "metadata": {
        "id": "LrNgRqvJAxdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_factorization(user_item, rank):\n",
        "    \"\"\"Runs matrix factorization on `user_item` and get user-item similarities.\n",
        "\n",
        "    Args:\n",
        "        user_item: User-item connectivity matrix.\n",
        "        rank: Number of numbers to represent a user / item.\n",
        "\n",
        "    Returns:\n",
        "        User-item similarities.\n",
        "    \"\"\"\n",
        "    weights, (user_factors, item_factors) = \\\n",
        "        decomposition.parafac(user_item, rank)\n",
        "    similarities = user_factors @ item_factors.T\n",
        "    return 1 / (1 + np.exp(- similarities))"
      ],
      "metadata": {
        "id": "ivMeZ8SP6sAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that our LightGCN model delivers betters performance than matrix factorization."
      ],
      "metadata": {
        "id": "Z0GhZI3cQ7D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute baseline metrics using matrix factorization.\n",
        "baseline_pred = matrix_factorization(\n",
        "        data[\"edge_index\"].detach().cpu().numpy(),\n",
        "        config_dict[\"mf_rank\"])[user_indices.cpu(), item_indices.cpu()]\n",
        "baseline_topk_precision, baseline_topk_recall = \\\n",
        "        personalized_topk(baseline_pred, K, user_indices, data[\"edge_index\"])\n",
        "print(\"Baseline (PARAFAC matrix factorization) produces \",\n",
        "      \"Top K precision = {}, recall = {}.\".format(baseline_topk_precision,\n",
        "                                                  baseline_topk_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G30yzPzf6A36",
        "outputId": "6e724538-450f-4bdd-d1c6-b855d6689335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline (PARAFAC matrix factorization) produces  Top K precision = 0.03499999999999999, recall = 0.0024017040639482414.\n"
          ]
        }
      ]
    }
  ]
}